{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Of The Important Libraries Required for Solving This Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import graphviz\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untitled.ipynb',\n",
       " 'model.png',\n",
       " '.ipynb_checkpoints',\n",
       " 'negative.txt',\n",
       " 'embedd_cnn_model.h5',\n",
       " 'Sentiment_Analysis_Udacity.ipynb',\n",
       " 'txt_sentoken',\n",
       " 'vocab.txt',\n",
       " 'review_polarity',\n",
       " 'positive.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step \n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load document into the notebook\n",
    "def load_document(fileName):\n",
    "    file=open(fileName,'r')\n",
    "    text_data=file.read()\n",
    "    file.close()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Step\n",
    "# Turn a document into tokens after processing it.\n",
    "\n",
    "def clean_document(document,m_type=\"mlp\"):\n",
    "    #split the review into tokens by white space\n",
    "    tokens=document.split()\n",
    "    # regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # removetokens which are not alphabetis\n",
    "    if m_type==\"mlp\":\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # remove stop words\n",
    "        ##  A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # remove out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'movie', 'comes', 'along', 'suspect', 'studio', 'every', 'indication', 'stinker', 'everybodys', 'surprise', 'perhaps', 'even', 'studio', 'film', 'becomes', 'critical', 'darling', 'mtv', 'films', 'election', 'high', 'school', 'comedy', 'starring', 'matthew', 'broderick', 'reese', 'witherspoon', 'current', 'example', 'anybody', 'know', 'film', 'existed', 'week', 'opened', 'plot', 'deceptively', 'simple', 'george', 'washington', 'carver', 'high', 'school', 'student', 'elections', 'tracy', 'flick', 'reese', 'witherspoon', 'overachiever', 'hand', 'raised', 'nearly', 'every', 'question', 'way', 'way', 'high', 'mr', 'matthew', 'broderick', 'sick', 'megalomaniac', 'student', 'encourages', 'paul', 'popularbutslow', 'jock', 'run', 'pauls', 'nihilistic', 'sister', 'jumps', 'race', 'well', 'personal', 'reasons', 'dark', 'side', 'sleeper', 'success', 'expectations', 'low', 'going', 'fact', 'quality', 'stuff', 'made', 'reviews', 'even', 'enthusiastic', 'right', 'cant', 'help', 'going', 'baggage', 'glowing', 'reviews', 'contrast', 'negative', 'baggage', 'reviewers', 'likely', 'election', 'good', 'film', 'live', 'hype', 'makes', 'election', 'disappointing', 'contains', 'significant', 'plot', 'details', 'lifted', 'directly', 'rushmore', 'released', 'months', 'earlier', 'similarities', 'staggering', 'tracy', 'flick', 'election', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'max', 'fischer', 'rushmore', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'significant', 'tension', 'election', 'potential', 'relationship', 'teacher', 'student', 'significant', 'tension', 'rushmore', 'potential', 'relationship', 'teacher', 'student', 'tracy', 'flick', 'single', 'parent', 'home', 'contributed', 'drive', 'max', 'fischer', 'single', 'parent', 'home', 'contributed', 'drive', 'male', 'bumbling', 'adult', 'election', 'matthew', 'broderick', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'even', 'gets', 'bee', 'sting', 'male', 'bumbling', 'adult', 'rushmore', 'bill', 'murray', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'gets', 'several', 'bee', 'stings', 'happened', 'individual', 'screenplay', 'rushmore', 'novel', 'election', 'contain', 'many', 'significant', 'plot', 'points', 'yet', 'films', 'probably', 'even', 'aware', 'made', 'two', 'different', 'studios', 'genre', 'high', 'school', 'geeks', 'revenge', 'movie', 'hadnt', 'fully', 'formed', 'yet', 'even', 'strengths', 'election', 'rely', 'upon', 'fantastic', 'performances', 'broderick', 'witherspoon', 'newcomer', 'jessica', 'campbell', 'pauls', 'antisocial', 'sister', 'tammy', 'broderick', 'playing', 'mr', 'rooney', 'role', 'ferris', 'bueller', 'seems', 'fun', 'hes', 'since', 'witherspoon', 'revelation', 'early', 'year', 'comedy', 'teenagers', 'little', 'clout', 'money', 'witherspoon', 'deserves', 'oscar', 'nomination', 'campbells', 'character', 'gets', 'going', 'like', 'fantastic', 'speech', 'gymnasium', 'youre', 'one', 'thing', 'thats', 'bothering', 'since', 'ive', 'seen', 'extraordinary', 'amount', 'sexuality', 'film', 'suppose', 'coming', 'mtv', 'films', 'expect', 'less', 'film', 'starts', 'light', 'airy', 'like', 'sitcom', 'screws', 'tighten', 'tensions', 'mount', 'alexander', 'payne', 'decides', 'add', 'elements', 'frankly', 'distract', 'story', 'bad', 'enough', 'mr', 'doesnt', 'like', 'tracys', 'determination', 'win', 'costs', 'throw', 'studentteacher', 'relationship', 'even', 'theres', 'logical', 'reason', 'mr', 'affair', 'theres', 'lot', 'like', 'election', 'plot', 'similarities', 'rushmore', 'tonal', 'nosedive', 'takes', 'gets', 'explicitly', 'sexdriven', 'mark', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_sentoken/pos/cv001_18431.txt\"\n",
    "text=load_document(filename)\n",
    "tokens=clean_document(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory='txt_sentoken/neg'\n",
    "# for file in os.listdir(directory):\n",
    "#     if file.endswith(\".txt\"):\n",
    "#         doc=load_document(directory+'/'+file)\n",
    "#         print(\"Loaded Document %s\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Document and then add the words to Vocab\n",
    "def document_to_vocabulary(fileName,vocab):\n",
    "    document=load_document(fileName)\n",
    "    tokens=clean_document(document)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Process All the documents in the Directory\n",
    "def process_documents(directory, vocab):\n",
    "    count=0\n",
    "    for fileName in os.listdir(directory):\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            path=directory+\"/\"+fileName\n",
    "            document_to_vocabulary(path,vocab)\n",
    "            count+=1\n",
    "    print(\"Total Number Of Files Processed In {d} = {n}\".format(d=directory,n=count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Files Processed In txt_sentoken/neg = 1000\n",
      "Total Number Of Files Processed In txt_sentoken/pos = 1000\n"
     ]
    }
   ],
   "source": [
    "# Main Function To Process the documents \n",
    "# Global Varaible To Store The Count \n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def develop_vocab():\n",
    "    global vocab\n",
    "#     vocab= Counter()\n",
    "    process_documents('txt_sentoken/neg', vocab)\n",
    "    process_documents('txt_sentoken/pos', vocab)\n",
    "    \n",
    "    \n",
    "    min_occur = 5\n",
    "    \n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "    save_list(tokens,\"vocab.txt\")\n",
    "\n",
    "develop_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "#Check Whether The File is Create or not \n",
    "if \"vocab.txt\" in os.listdir():\n",
    "    print(\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length Of The Vocabulary 46557\n"
     ]
    }
   ],
   "source": [
    "#print Length of the Vocabulary\n",
    "print(\"Total Length Of The Vocabulary %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "# 50 Most Common Words\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Ten Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe9d2d06a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabmcdf=pd.DataFrame(data=vocab.most_common(10),columns=['Word','Count'])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x='Word',y=\"Count\",data=vocabmcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words in Vocab.txt is 14803\n"
     ]
    }
   ],
   "source": [
    "#Load The Vocabulary from Vocab.txt File.\n",
    "\n",
    "vocab_data=load_document(\"vocab.txt\")\n",
    "vocab_data=vocab_data.split()\n",
    "vocab=set(vocab_data)\n",
    "print(\"Number of Words in Vocab.txt is %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Negative.txt and Positive.txt from all the review documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review_documents(directory,vocab,train=False,op=0,m_type=\"mlp\"):\n",
    "    lines=[]\n",
    "    count=0\n",
    "    for file in os.listdir(directory):\n",
    "#         print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            if not op:\n",
    "                if train and file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "                if not train and not file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "            count=count+1\n",
    "            file_path=directory + '/' + file\n",
    "            lines_in_file=load_document(file_path)\n",
    "            tokens=clean_document(lines_in_file,m_type)\n",
    "            tokens=[w for w in tokens if w in vocab]\n",
    "            line=' '.join(tokens)\n",
    "        lines.append(line)\n",
    "#         print(lines)\n",
    "    print(\"Number of File Processed in {d} is {n}\".format(d=directory,n=count))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 1000\n",
      "Length of the Negative.txt File is 2124581 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "neg_lines=process_review_documents(\"txt_sentoken/neg\",vocab_data,op=1)\n",
    "save_list(neg_lines,\"negative.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txt\n",
    "negative_lines=load_document(\"negative.txt\")\n",
    "print(\"Length of the Negative.txt File is %s \" %len(negative_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studio attracted many weird bizarre people gates wonder film life death studio centers one boring cl\n"
     ]
    }
   ],
   "source": [
    "print(negative_lines[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive lines from all the documents from \"txt_sentoken/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/pos is 1000\n",
      "Length of the Positive.txt File is 2408780 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "pos_lines=process_review_documents(\"txt_sentoken/pos\",vocab_data,op=1)\n",
    "save_list(pos_lines,\"positive.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txr\n",
    "positive_lines=load_document(\"positive.txt\")\n",
    "print(\"Length of the Positive.txt File is %s \" %len(positive_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david lynchs blue velvet begins ends colorful bright shots flowers happy americans seemingly perfect\n"
     ]
    }
   ],
   "source": [
    "print(positive_lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,op,train=False,m_type=\"mlp\"):\n",
    "    neg=process_review_documents('txt_sentoken/neg', vocab,train,op,m_type)\n",
    "    pos=process_review_documents('txt_sentoken/pos', vocab,train,op,m_type)\n",
    "    docs=neg+pos\n",
    "    print(\"Length of   Negative Files = {n} \\t Positive Files= {p} \\t Doc = {d} \".format(n=len(neg),p=len(pos),d=len(docs)))\n",
    "    labels =[0 for i in range(len(neg))] + [1 for j in range(len(pos))]\n",
    "    return docs,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Dividing 90% 10% Ration 1000 Review Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n"
     ]
    }
   ],
   "source": [
    "# load all training reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab,train=True,op=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "test_docs, ytest = load_clean_dataset(vocab,train=False,op=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenizer To Implement Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To Tokenize\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 14781) (200, 14781)\n"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14781"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words=Xtest.shape[1]\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ashish/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                369550    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 369,576\n",
      "Trainable params: 369,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ashish/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6923 - acc: 0.4983\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6870 - acc: 0.5361\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6772 - acc: 0.8683\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6619 - acc: 0.8217\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6415 - acc: 0.9256\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6172 - acc: 0.9222\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.5895 - acc: 0.9306\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5610 - acc: 0.9322\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.5308 - acc: 0.9367\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.4999 - acc: 0.9422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d24a8438>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit network\n",
    "model=define_model(n_words)\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 160.00 191.00\" width=\"160pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 156,-187 156,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140642232205944 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140642232205944</title>\n",
       "<polygon fill=\"none\" points=\"12,-73.5 12,-109.5 140,-109.5 140,-73.5 12,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140642232136872 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140642232136872</title>\n",
       "<polygon fill=\"none\" points=\"12,-.5 12,-36.5 140,-36.5 140,-.5 12,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140642232205944&#45;&gt;140642232136872 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140642232205944-&gt;140642232136872</title>\n",
       "<path d=\"M76,-73.4551C76,-65.3828 76,-55.6764 76,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-46.5903 76,-36.5904 72.5001,-46.5904 79.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140642232207792 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140642232207792</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 152,-182.5 152,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-160.8\">140642232207792</text>\n",
       "</g>\n",
       "<!-- 140642232207792&#45;&gt;140642232205944 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140642232207792-&gt;140642232205944</title>\n",
       "<path d=\"M76,-146.4551C76,-138.3828 76,-128.6764 76,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-119.5903 76,-109.5904 72.5001,-119.5904 79.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 55.776715\n",
      "Test Accuracy: 83.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc  = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (loss*100))\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment For Reviews For Benchmark Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE \n",
      "\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE \n",
      "\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE \n",
      "\n",
      "Review: [Very heart touching and making us proud movie. The acting done by akshay Kumar is awesome in the movie.   The whole theater was emotional at the end of the movie.]\n",
      "Sentiment: NEGATIVE \n",
      "\n",
      "Review: [one Of the Best Movie evar made in Bollywood on War What a wonderful Location and Sets Akshay Kumar Naild it Again and proven again that why He is king Of the Bollywood]\n",
      "Sentiment: POSITIVE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLP\n",
    "#\n",
    "m_type=\"mlp\"\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s \\n' % (text, sentiment))\n",
    "# test negative text\n",
    "\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s \\n' % (text, sentiment))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s \\n' % (text, sentiment))\n",
    "\n",
    "\n",
    "text = \"Very heart touching and making us proud movie. The acting done \\\n",
    "by akshay Kumar is awesome in the movie.   \\\n",
    "The whole theater was emotional at the end of the movie.\"\n",
    "\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s \\n' % (text, sentiment))\n",
    "\n",
    "text = \"one Of the Best Movie evar made in Bollywood on War What a wonderful \\\n",
    "Location and Sets Akshay Kumar Naild it Again and proven again that why He is king Of the Bollywood\"\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s \\n' % (text, sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Embedding Layer Model\n",
    "\n",
    "I will test different options to see which gives the best result on this problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN With Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "cnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"cnn\")\n",
    "cnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1244\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in cnn_train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(tokenizer ,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embed_cnn_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 200, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 14781\n"
     ]
    }
   ],
   "source": [
    "cnn_tokenizer=create_tokenizer(cnn_train_docs)\n",
    "vocabulary_size=len(cnn_tokenizer.word_index)+1\n",
    "print(\"Vocabulary Size %d\"%vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "cnn_Xtrain = encode_documents(cnn_tokenizer, max_length, cnn_train_docs)\n",
    "cnn_Xtest = encode_documents(cnn_tokenizer, max_length, cnn_test_docs)\n",
    "\n",
    "print(len(cnn_Xtrain))\n",
    "print(len(cnn_Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1244, 200)         2956200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1237, 32)          51232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 618, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                197770    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 3,205,213\n",
      "Trainable params: 3,205,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_cnn_model=define_embed_cnn_model(vocabulary_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 262.00 483.00\" width=\"262pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 258,-479 258,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140642230041624 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140642230041624</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-365.5 25.5,-401.5 228.5,-401.5 228.5,-365.5 25.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-379.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140641974958064 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140641974958064</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-292.5 51.5,-328.5 202.5,-328.5 202.5,-292.5 51.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-306.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140642230041624&#45;&gt;140641974958064 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140642230041624-&gt;140641974958064</title>\n",
       "<path d=\"M127,-365.4551C127,-357.3828 127,-347.6764 127,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-338.5903 127,-328.5904 123.5001,-338.5904 130.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140641974957000 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140641974957000</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 254,-255.5 254,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-233.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140641974958064&#45;&gt;140641974957000 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140641974958064-&gt;140641974957000</title>\n",
       "<path d=\"M127,-292.4551C127,-284.3828 127,-274.6764 127,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-265.5903 127,-255.5904 123.5001,-265.5904 130.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140641930161792 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140641930161792</title>\n",
       "<polygon fill=\"none\" points=\"56.5,-146.5 56.5,-182.5 197.5,-182.5 197.5,-146.5 56.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-160.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140641974957000&#45;&gt;140641930161792 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140641974957000-&gt;140641930161792</title>\n",
       "<path d=\"M127,-219.4551C127,-211.3828 127,-201.6764 127,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-192.5903 127,-182.5904 123.5001,-192.5904 130.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140641974486520 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140641974486520</title>\n",
       "<polygon fill=\"none\" points=\"63,-73.5 63,-109.5 191,-109.5 191,-73.5 63,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140641930161792&#45;&gt;140641974486520 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140641930161792-&gt;140641974486520</title>\n",
       "<path d=\"M127,-146.4551C127,-138.3828 127,-128.6764 127,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-119.5903 127,-109.5904 123.5001,-119.5904 130.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140641974601600 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140641974601600</title>\n",
       "<polygon fill=\"none\" points=\"63,-.5 63,-36.5 191,-36.5 191,-.5 63,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140641974486520&#45;&gt;140641974601600 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140641974486520-&gt;140641974601600</title>\n",
       "<path d=\"M127,-73.4551C127,-65.3828 127,-55.6764 127,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-46.5903 127,-36.5904 123.5001,-46.5904 130.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140641974897968 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140641974897968</title>\n",
       "<polygon fill=\"none\" points=\"51,-438.5 51,-474.5 203,-474.5 203,-438.5 51,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-452.8\">140641974897968</text>\n",
       "</g>\n",
       "<!-- 140641974897968&#45;&gt;140642230041624 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140641974897968-&gt;140642230041624</title>\n",
       "<path d=\"M127,-438.4551C127,-430.3828 127,-420.6764 127,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-411.5903 127,-401.5904 123.5001,-411.5904 130.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(embed_cnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 29s - loss: 1.4461e-04 - acc: 1.0000\n",
      "Epoch 2/10\n",
      " - 35s - loss: 1.1213e-04 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " - 36s - loss: 8.9307e-05 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 28s - loss: 7.1453e-05 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 29s - loss: 5.9499e-05 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 27s - loss: 5.0063e-05 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 29s - loss: 4.3239e-05 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 27s - loss: 3.7328e-05 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 27s - loss: 3.2766e-05 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 36s - loss: 2.8890e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9b078db38>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_cnn_model.fit(cnn_Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Cnn Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002658\n",
      "Train Accuracy: 100.000000\n",
      "\n",
      "Test Loss: 62.780445\n",
      "Test Accuracy: 86.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate Train Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtrain, ytrain, verbose=0)\n",
    "print('Train Loss: %f' % (cnn_loss*100))\n",
    "print('Train Accuracy: %f\\n' % (cnn_acc*100))\n",
    "\n",
    "# evaluate Test Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (cnn_loss*100))\n",
    "print('Test Accuracy: %f' % (cnn_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('embedd_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: POSITIVE (52.327%)\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (59.774%)\n",
      "Review: [Very heart touching and making us proud movie. The acting done by akshay Kumar is awesome in the movie.   The whole theater was emotional at the end of the movie.]\n",
      "Sentiment: NEGATIVE (69.987%)\n",
      "Review: [I loved This Movie. I would recommend this movie to everyone]\n",
      "Sentiment: POSITIVE (52.309%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"Very heart touching and making us proud movie. The acting done \\\n",
    "by akshay Kumar is awesome in the movie.   \\\n",
    "The whole theater was emotional at the end of the movie.\"\n",
    "\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie. I would recommend this movie to everyone.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Word Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes= ['binary', 'count', 'tfidf', 'freq']\n",
    "# #Prepare Bag - Of - Words Encoding\n",
    "# results=pd.DataFrame()\n",
    "# def prepare_data(train,test,m):\n",
    "#     token=Tokenizer()\n",
    "#     token.fit_on_texts(train)\n",
    "#     Xtrain=token.texts_to_matrix(train, mode=mode)\n",
    "#     Xtest = token.texts_to_matrix(test, mode=mode)\n",
    "#     return Xtrain,Xtest\n",
    "\n",
    "# #Evaluate\n",
    "# def evaluate_mode(Xtrain,ytrain,Xtest,ytest):\n",
    "#     scores=list()\n",
    "#     no_r=10\n",
    "#     for _ in range(no_r):\n",
    "#         model=Sequential() \n",
    "#         model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "#         model.add(Dense(1,activation='sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#         model.fit(Xtrain,ytrain,epochs=10,verbose=1)\n",
    "#         loss,acc=model.evaluate(Xtest,ytest,verbose=1)\n",
    "#         scores.append(acc)\n",
    "#         print(\"%d accuracy : %s \"%((_+1),acc))\n",
    "#     return scores\n",
    "\n",
    "# for mode in modes:\n",
    "#     Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "#     results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.describe())\n",
    "# plt.figure(figsize=(16,6))\n",
    "# results.boxplot()\n",
    "# plt.xlabel(\"Types\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
