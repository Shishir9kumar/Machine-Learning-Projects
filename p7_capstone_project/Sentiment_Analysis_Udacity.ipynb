{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Of The Important Libraries Required for Solving This Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import graphviz\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.png',\n",
       " 'ncnn_model.h5',\n",
       " '.ipynb_checkpoints',\n",
       " 'negative.txt',\n",
       " 'mlp.ipynb',\n",
       " 'Sentiment_Analysis_Udacity.ipynb',\n",
       " 'txt_sentoken',\n",
       " 'vocab.txt',\n",
       " 'rp.tar.gz',\n",
       " 'review_polarity',\n",
       " 'positive.txt',\n",
       " 'cnn_model.png',\n",
       " 'cnn']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step \n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load document into the notebook\n",
    "def load_document(fileName):\n",
    "    file=open(fileName,'r')\n",
    "    text_data=file.read()\n",
    "    file.close()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Step\n",
    "# Turn a document into tokens after processing it.\n",
    "\n",
    "def clean_document(document,m_type=\"mlp\"):\n",
    "    document=document.lower()\n",
    "    #split the review into tokens by white space\n",
    "    tokens=document.split()\n",
    "    # regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # removetokens which are not alphabetis\n",
    "    if m_type==\"mlp\":\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # remove stop words\n",
    "        ##  A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # remove out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'movie', 'comes', 'along', 'suspect', 'studio', 'every', 'indication', 'stinker', 'everybodys', 'surprise', 'perhaps', 'even', 'studio', 'film', 'becomes', 'critical', 'darling', 'mtv', 'films', 'election', 'high', 'school', 'comedy', 'starring', 'matthew', 'broderick', 'reese', 'witherspoon', 'current', 'example', 'anybody', 'know', 'film', 'existed', 'week', 'opened', 'plot', 'deceptively', 'simple', 'george', 'washington', 'carver', 'high', 'school', 'student', 'elections', 'tracy', 'flick', 'reese', 'witherspoon', 'overachiever', 'hand', 'raised', 'nearly', 'every', 'question', 'way', 'way', 'high', 'mr', 'matthew', 'broderick', 'sick', 'megalomaniac', 'student', 'encourages', 'paul', 'popularbutslow', 'jock', 'run', 'pauls', 'nihilistic', 'sister', 'jumps', 'race', 'well', 'personal', 'reasons', 'dark', 'side', 'sleeper', 'success', 'expectations', 'low', 'going', 'fact', 'quality', 'stuff', 'made', 'reviews', 'even', 'enthusiastic', 'right', 'cant', 'help', 'going', 'baggage', 'glowing', 'reviews', 'contrast', 'negative', 'baggage', 'reviewers', 'likely', 'election', 'good', 'film', 'live', 'hype', 'makes', 'election', 'disappointing', 'contains', 'significant', 'plot', 'details', 'lifted', 'directly', 'rushmore', 'released', 'months', 'earlier', 'similarities', 'staggering', 'tracy', 'flick', 'election', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'max', 'fischer', 'rushmore', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'significant', 'tension', 'election', 'potential', 'relationship', 'teacher', 'student', 'significant', 'tension', 'rushmore', 'potential', 'relationship', 'teacher', 'student', 'tracy', 'flick', 'single', 'parent', 'home', 'contributed', 'drive', 'max', 'fischer', 'single', 'parent', 'home', 'contributed', 'drive', 'male', 'bumbling', 'adult', 'election', 'matthew', 'broderick', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'even', 'gets', 'bee', 'sting', 'male', 'bumbling', 'adult', 'rushmore', 'bill', 'murray', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'gets', 'several', 'bee', 'stings', 'happened', 'individual', 'screenplay', 'rushmore', 'novel', 'election', 'contain', 'many', 'significant', 'plot', 'points', 'yet', 'films', 'probably', 'even', 'aware', 'made', 'two', 'different', 'studios', 'genre', 'high', 'school', 'geeks', 'revenge', 'movie', 'hadnt', 'fully', 'formed', 'yet', 'even', 'strengths', 'election', 'rely', 'upon', 'fantastic', 'performances', 'broderick', 'witherspoon', 'newcomer', 'jessica', 'campbell', 'pauls', 'antisocial', 'sister', 'tammy', 'broderick', 'playing', 'mr', 'rooney', 'role', 'ferris', 'bueller', 'seems', 'fun', 'hes', 'since', 'witherspoon', 'revelation', 'early', 'year', 'comedy', 'teenagers', 'little', 'clout', 'money', 'witherspoon', 'deserves', 'oscar', 'nomination', 'campbells', 'character', 'gets', 'going', 'like', 'fantastic', 'speech', 'gymnasium', 'youre', 'one', 'thing', 'thats', 'bothering', 'since', 'ive', 'seen', 'extraordinary', 'amount', 'sexuality', 'film', 'suppose', 'coming', 'mtv', 'films', 'expect', 'less', 'film', 'starts', 'light', 'airy', 'like', 'sitcom', 'screws', 'tighten', 'tensions', 'mount', 'alexander', 'payne', 'decides', 'add', 'elements', 'frankly', 'distract', 'story', 'bad', 'enough', 'mr', 'doesnt', 'like', 'tracys', 'determination', 'win', 'costs', 'throw', 'studentteacher', 'relationship', 'even', 'theres', 'logical', 'reason', 'mr', 'affair', 'theres', 'lot', 'like', 'election', 'plot', 'similarities', 'rushmore', 'tonal', 'nosedive', 'takes', 'gets', 'explicitly', 'sexdriven', 'mark', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_sentoken/pos/cv001_18431.txt\"\n",
    "text=load_document(filename)\n",
    "tokens=clean_document(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory='txt_sentoken/neg'\n",
    "# for file in os.listdir(directory):\n",
    "#     if file.endswith(\".txt\"):\n",
    "#         doc=load_document(directory+'/'+file)\n",
    "#         print(\"Loaded Document %s\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Document and then add the words to Vocab\n",
    "def document_to_vocabulary(fileName,vocab):\n",
    "    document=load_document(fileName)\n",
    "    tokens=clean_document(document)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Process All the documents in the Directory\n",
    "def process_documents(directory, vocab):\n",
    "    count=0\n",
    "    for fileName in os.listdir(directory):\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            path=directory+\"/\"+fileName\n",
    "            document_to_vocabulary(path,vocab)\n",
    "            count+=1\n",
    "    print(\"Total Number Of Files Processed In {d} = {n}\".format(d=directory,n=count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Files Processed In txt_sentoken/neg = 1000\n",
      "Total Number Of Files Processed In txt_sentoken/pos = 1000\n"
     ]
    }
   ],
   "source": [
    "# Main Function To Process the documents \n",
    "# Global Varaible To Store The Count \n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def develop_vocab():\n",
    "    global vocab\n",
    "#     vocab= Counter()\n",
    "    process_documents('txt_sentoken/neg', vocab)\n",
    "    process_documents('txt_sentoken/pos', vocab)\n",
    "    \n",
    "    \n",
    "    min_occur = 5\n",
    "    \n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "    save_list(tokens,\"vocab.txt\")\n",
    "\n",
    "develop_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "#Check Whether The File is Create or not \n",
    "if \"vocab.txt\" in os.listdir():\n",
    "    print(\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length Of The Vocabulary 46557\n"
     ]
    }
   ],
   "source": [
    "#print Length of the Vocabulary\n",
    "print(\"Total Length Of The Vocabulary %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "# 50 Most Common Words\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Ten Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb5760b198>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabmcdf=pd.DataFrame(data=vocab.most_common(10),columns=['Word','Count'])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x='Word',y=\"Count\",data=vocabmcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words in Vocab.txt is 14803\n"
     ]
    }
   ],
   "source": [
    "#Load The Vocabulary from Vocab.txt File.\n",
    "\n",
    "vocab_data=load_document(\"vocab.txt\")\n",
    "vocab_data=vocab_data.split()\n",
    "vocab=set(vocab_data)\n",
    "print(\"Number of Words in Vocab.txt is %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Negative.txt and Positive.txt from all the review documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review_documents(directory,vocab,train=False,op=0,m_type=\"mlp\"):\n",
    "    lines=[]\n",
    "    count=0\n",
    "    for file in os.listdir(directory):\n",
    "#         print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            if not op:\n",
    "                if train and file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "                if not train and not file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "            count=count+1\n",
    "            file_path=directory + '/' + file\n",
    "            lines_in_file=load_document(file_path)\n",
    "            tokens=clean_document(lines_in_file,m_type)\n",
    "            tokens=[w for w in tokens if w in vocab]\n",
    "            line=' '.join(tokens)\n",
    "        lines.append(line)\n",
    "#         print(lines)\n",
    "    print(\"Number of File Processed in {d} is {n}\".format(d=directory,n=count))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 1000\n",
      "Length of the Negative.txt File is 2124581 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "neg_lines=process_review_documents(\"txt_sentoken/neg\",vocab_data,op=1)\n",
    "save_list(neg_lines,\"negative.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txt\n",
    "negative_lines=load_document(\"negative.txt\")\n",
    "print(\"Length of the Negative.txt File is %s \" %len(negative_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studio attracted many weird bizarre people gates wonder film life death studio centers one boring cl\n"
     ]
    }
   ],
   "source": [
    "print(negative_lines[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive lines from all the documents from \"txt_sentoken/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/pos is 1000\n",
      "Length of the Positive.txt File is 2408780 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "pos_lines=process_review_documents(\"txt_sentoken/pos\",vocab_data,op=1)\n",
    "save_list(pos_lines,\"positive.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txr\n",
    "positive_lines=load_document(\"positive.txt\")\n",
    "print(\"Length of the Positive.txt File is %s \" %len(positive_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david lynchs blue velvet begins ends colorful bright shots flowers happy americans seemingly perfect\n"
     ]
    }
   ],
   "source": [
    "print(positive_lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,op,train=False,m_type=\"mlp\"):\n",
    "    neg=process_review_documents('txt_sentoken/neg', vocab,train,op,m_type)\n",
    "    pos=process_review_documents('txt_sentoken/pos', vocab,train,op,m_type)\n",
    "    docs=neg+pos\n",
    "    print(\"Length of   Negative Files = {n} \\t Positive Files= {p} \\t Doc = {d} \".format(n=len(neg),p=len(pos),d=len(docs)))\n",
    "    labels =[0 for i in range(len(neg))] + [1 for j in range(len(pos))]\n",
    "    return docs,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Dividing 90% 10% Ration 1000 Review Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n"
     ]
    }
   ],
   "source": [
    "# load all training reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab,train=True,op=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "test_docs, ytest = load_clean_dataset(vocab,train=False,op=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenizer To Implement Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To Tokenize\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 14781) (200, 14781)\n"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14781"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words=Xtest.shape[1]\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 25)                369550    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 369,576\n",
      "Trainable params: 369,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#fit network\n",
    "model=define_model(n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.6917 - acc: 0.5989\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 1s 412us/step - loss: 0.6841 - acc: 0.8711\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 1s 371us/step - loss: 0.6721 - acc: 0.8572\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 1s 394us/step - loss: 0.6549 - acc: 0.8611\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 1s 403us/step - loss: 0.6323 - acc: 0.9206\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 1s 395us/step - loss: 0.6056 - acc: 0.9233\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 1s 402us/step - loss: 0.5766 - acc: 0.9256\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 1s 395us/step - loss: 0.5449 - acc: 0.9356\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 1s 392us/step - loss: 0.5129 - acc: 0.9400\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 1s 405us/step - loss: 0.4814 - acc: 0.9433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb9dc21358>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 160.00 191.00\" width=\"160pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 156,-187 156,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443782468440 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443782468440</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-73.5 7.5,-109.5 144.5,-109.5 144.5,-73.5 7.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-87.8\">dense_41: Dense</text>\n",
       "</g>\n",
       "<!-- 140443782468664 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443782468664</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-.5 7.5,-36.5 144.5,-36.5 144.5,-.5 7.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-14.8\">dense_42: Dense</text>\n",
       "</g>\n",
       "<!-- 140443782468440&#45;&gt;140443782468664 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443782468440-&gt;140443782468664</title>\n",
       "<path d=\"M76,-73.4551C76,-65.3828 76,-55.6764 76,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-46.5903 76,-36.5904 72.5001,-46.5904 79.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443782467824 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443782467824</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 152,-182.5 152,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-160.8\">140443782467824</text>\n",
       "</g>\n",
       "<!-- 140443782467824&#45;&gt;140443782468440 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443782467824-&gt;140443782468440</title>\n",
       "<path d=\"M76,-146.4551C76,-138.3828 76,-128.6764 76,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-119.5903 76,-109.5904 72.5001,-119.5904 79.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 54.455891\n",
      "Test Accuracy: 86.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc  = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (loss*100))\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment For Reviews For Benchmark Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "#     if percent_pos < 0.5:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#             if percent_pos >0.5 and percent_pos<0.505:\n",
    "#                 return percent_pos, 'NEUTRAL'\n",
    "#             else:\n",
    "#                 return percent_pos, 'POSITIVE'\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: NEGATIVE (70.844%) \n",
      "\n",
      "best movie ever great recommend\n",
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (82.057%) \n",
      "\n",
      "bad movie\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (99.110%) \n",
      "\n",
      "average one one time watch\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE (51.494%) \n",
      "\n",
      "give break top movie time even close bad guy movie one worst characters ever seen good flick tried build love peter parker girl sudden cant please movie become cult movie get good rating people afraid speak truth movie wasnt feel sequel might better dont build character much\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (80.554%) \n",
      "\n",
      "ok greatest comic book movie still prefer original batman movie superman movie cool scenes overall disappointed tobey pretty good kirsten liked virgin suicide wasnt digging role fell flat opinion\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: NEGATIVE (61.514%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLP\n",
    "#\n",
    "m_type=\"mlp\"\n",
    "\n",
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Embedding Layer Model\n",
    "\n",
    "I will test different options to see which gives the best result on this problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN With Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "cnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"cnn\")\n",
    "cnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1244\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in cnn_train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(tokenizer ,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embed_cnn_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     filepath=\"weights.best.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor= 'val_acc' , verbose=1, save_best_only=True,\n",
    "#     mode= max )\n",
    "#     callbacks_list = [checkpoint]\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='cnn_model.png', show_shapes=True)\n",
    "#     return model,callbacks_list\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 14781\n"
     ]
    }
   ],
   "source": [
    "cnn_tokenizer=create_tokenizer(cnn_train_docs)\n",
    "vocabulary_size=len(cnn_tokenizer.word_index)+1\n",
    "print(\"Vocabulary Size %d\"%vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "#     tokens = clean_document(review)\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     line = ' '.join(tokens)\n",
    "#     padded=encode_documents(tokenizer,max_length,[line])\n",
    "#     # predict sentiment\n",
    "#     yhat = model.predict(padded, verbose=0)\n",
    "#     percent_pos = yhat[0,0]\n",
    "#     print(percent_pos)\n",
    "#     if percent_pos < 0.49:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "#             return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "cnn_Xtrain = encode_documents(cnn_tokenizer, max_length, cnn_train_docs)\n",
    "cnn_Xtest = encode_documents(cnn_tokenizer, max_length, cnn_test_docs)\n",
    "\n",
    "print(len(cnn_Xtrain))\n",
    "print(len(cnn_Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1244, 100)         1478100   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1237, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 618, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                197770    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,701,513\n",
      "Trainable params: 1,701,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_cnn_model=define_embed_cnn_model(vocabulary_size,max_length)\n",
    "\n",
    "# embed_cnn_model,callb=define_embed_cnn_model(vocabulary_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 262.00 483.00\" width=\"262pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 258,-479 258,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443357897504 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443357897504</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-365.5 25.5,-401.5 228.5,-401.5 228.5,-365.5 25.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-379.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443357898512 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443357898512</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-292.5 51.5,-328.5 202.5,-328.5 202.5,-292.5 51.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-306.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443357897504&#45;&gt;140443357898512 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443357897504-&gt;140443357898512</title>\n",
       "<path d=\"M127,-365.4551C127,-357.3828 127,-347.6764 127,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-338.5903 127,-328.5904 123.5001,-338.5904 130.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443804161528 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443804161528</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 254,-255.5 254,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-233.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443357898512&#45;&gt;140443804161528 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140443357898512-&gt;140443804161528</title>\n",
       "<path d=\"M127,-292.4551C127,-284.3828 127,-274.6764 127,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-265.5903 127,-255.5904 123.5001,-265.5904 130.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357900304 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140443357900304</title>\n",
       "<polygon fill=\"none\" points=\"56.5,-146.5 56.5,-182.5 197.5,-182.5 197.5,-146.5 56.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-160.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443804161528&#45;&gt;140443357900304 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140443804161528-&gt;140443357900304</title>\n",
       "<path d=\"M127,-219.4551C127,-211.3828 127,-201.6764 127,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-192.5903 127,-182.5904 123.5001,-192.5904 130.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443360553504 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140443360553504</title>\n",
       "<polygon fill=\"none\" points=\"63,-73.5 63,-109.5 191,-109.5 191,-73.5 63,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140443357900304&#45;&gt;140443360553504 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140443357900304-&gt;140443360553504</title>\n",
       "<path d=\"M127,-146.4551C127,-138.3828 127,-128.6764 127,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-119.5903 127,-109.5904 123.5001,-119.5904 130.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357465232 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140443357465232</title>\n",
       "<polygon fill=\"none\" points=\"63,-.5 63,-36.5 191,-36.5 191,-.5 63,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140443360553504&#45;&gt;140443357465232 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140443360553504-&gt;140443357465232</title>\n",
       "<path d=\"M127,-73.4551C127,-65.3828 127,-55.6764 127,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-46.5903 127,-36.5904 123.5001,-46.5904 130.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357898456 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140443357898456</title>\n",
       "<polygon fill=\"none\" points=\"51,-438.5 51,-474.5 203,-474.5 203,-438.5 51,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-452.8\">140443357898456</text>\n",
       "</g>\n",
       "<!-- 140443357898456&#45;&gt;140443357897504 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443357898456-&gt;140443357897504</title>\n",
       "<path d=\"M127,-438.4551C127,-430.3828 127,-420.6764 127,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-411.5903 127,-401.5904 123.5001,-411.5904 130.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(embed_cnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.6900 - acc: 0.5378\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 18s 10ms/step - loss: 0.5349 - acc: 0.7561\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.1252 - acc: 0.9578\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.0145 - acc: 0.9989\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 9.2583e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 7.4423e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 18s 10ms/step - loss: 6.2321e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb847312e8>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_cnn_model.fit(cnn_Xtrain, ytrain, epochs=10, verbose=1)\n",
    "\n",
    "# history=embed_cnn_model.fit(cnn_Xtrain, ytrain,validation_split=0.20 ,epochs=10, verbose=0,batch_size=100,callbacks=callb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Of Accuracy And Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# print(h`istory.history.keys())\n",
    "\n",
    "# # history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Cnn Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 4s 2ms/step\n",
      "Train Loss: 0.055193\n",
      "Train Accuracy: 100.000000\n",
      "\n",
      "200/200 [==============================] - 1s 3ms/step\n",
      "Test Loss: 49.719241\n",
      "Test Accuracy: 86.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate Train Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtrain, ytrain, verbose=1)\n",
    "print('Train Loss: %f' % (cnn_loss*100))\n",
    "print('Train Accuracy: %f\\n' % (cnn_acc*100))\n",
    "\n",
    "# evaluate Test Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtest, ytest, verbose=1)\n",
    "print('Test Loss: %f' % (cnn_loss*100))\n",
    "print('Test Accuracy: %f' % (cnn_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('embedd_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "#     if percent_pos < 0.49:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "#             return percent_pos, 'POSITIVE'\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "#     tokens = clean_document(review)\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     line = ' '.join(tokens)\n",
    "#     padded=encode_documents(tokenizer,max_length,[line])\n",
    "#     # predict sentiment\n",
    "#     yhat = model.predict(padded, verbose=0)\n",
    "#     percent_pos = yhat[0,0]\n",
    "#     print(percent_pos)\n",
    "#     if round(percent_pos) == 0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_cnn_model.load_weights(\"weights.best.hdf5\")\n",
    "# embed_cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.5109663\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: POSITIVE (51.097%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.4449024\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (55.510%) \n",
      " \n",
      "give break top movie time even close bad guy movie one worst characters ever seen good flick tried build love peter parker girl sudden cant please movie become cult movie get good rating people afraid speak truth movie wasnt feel sequel might better dont build character much\n",
      "0.18961701\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (81.038%) \n",
      "\n",
      "ok greatest comic book movie still prefer original batman movie superman movie cool scenes overall disappointed tobey pretty good kirsten liked virgin suicide wasnt digging role fell flat opinion\n",
      "0.73974013\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: POSITIVE (73.974%) \n",
      "\n",
      "goodness story ever told film would recommend movie everyone\n",
      "0.4999043\n",
      "Review: [The goodness of the  story ever told on film . I would recommend this movie to everyone .]\n",
      "Sentiment: POSITIVE (49.990%) \n",
      "\n",
      "average one one time watch\n",
      "0.5019089\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE (50.191%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n ' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"The goodness of the  story ever told on film . I would recommend this movie to everyone .\"\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everyone enjoy film love recommended\n",
      "0.513243\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: POSITIVE (51.324%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.4449024\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (55.510%) \n",
      "\n",
      "heart touching making us proud movie acting done awesome movie whole theater emotional end movie would recommend movie everyone\n",
      "0.28799298\n",
      "Review: [Very heart touching and making us proud movie. The acting done by akshay Kumar is awesome in the movie.   The whole theater was emotional at the end of the movie . I would recommend this movie to everyone.]\n",
      "Sentiment: NEGATIVE (71.201%) \n",
      "\n",
      "loved movie movie good would recommend movie everyone\n",
      "0.50684905\n",
      "Review: [I loved This Movie. This Movie is too good ,I would recommend this movie to everyone.]\n",
      "Sentiment: POSITIVE (50.685%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"Very heart touching and making us proud movie. The acting done \\\n",
    "by akshay Kumar is awesome in the movie.   \\\n",
    "The whole theater was emotional at the end of the movie . I would recommend this movie to everyone.\"\n",
    "\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n'  % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie. This Movie is too good ,I would recommend this movie to everyone.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Embedding Cnn Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "ncnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"ncnn\")\n",
    "ncnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"ncnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_tokenizer = create_tokenizer(ncnn_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1244\n",
      "Vocabulary size: 14781\n"
     ]
    }
   ],
   "source": [
    "length = max([len(s.split()) for s in ncnn_train_docs])\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(ncnn_tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_Xtrain = encode_documents(ncnn_tokenizer,length, ncnn_train_docs)\n",
    "ncnn_Xtest = encode_documents(ncnn_tokenizer, length,ncnn_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1244)\n",
      "(200, 1244)\n"
     ]
    }
   ],
   "source": [
    "print(ncnn_Xtrain.shape)\n",
    "print(ncnn_Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_ncnn_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "#     # channel 3\n",
    "#     inputs4 = Input(shape=(length,))\n",
    "#     embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "#     conv4 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding4)\n",
    "#     drop4 = Dropout(0.5)(conv4)\n",
    "#     pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "#     flat4 = Flatten()(pool4)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='ncnn_model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_44 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_45 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_46 (Embedding)        (None, 1244, 100)    1478100     input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_47 (Embedding)        (None, 1244, 100)    1478100     input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_48 (Embedding)        (None, 1244, 100)    1478100     input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 1241, 32)     12832       embedding_46[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1239, 32)     19232       embedding_47[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 1237, 32)     25632       embedding_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 1241, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 1239, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 1237, 32)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 620, 32)      0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 619, 32)      0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 618, 32)      0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_46 (Flatten)            (None, 19840)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_47 (Flatten)            (None, 19808)        0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_48 (Flatten)            (None, 19776)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 59424)        0           flatten_46[0][0]                 \n",
      "                                                                 flatten_47[0][0]                 \n",
      "                                                                 flatten_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 10)           594250      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1)            11          dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,086,257\n",
      "Trainable params: 5,086,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ncnn_model= define_ncnn_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ncnn_Xtrain.shape)\n",
    "# print(ncnn_Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 833.00 629.00\" width=\"833pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 829,-625 829,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443486434640 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443486434640</title>\n",
       "<polygon fill=\"none\" points=\"47,-584.5 47,-620.5 216,-620.5 216,-584.5 47,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-598.8\">input_44: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443486437104 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140443486437104</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-511.5 25.5,-547.5 237.5,-547.5 237.5,-511.5 25.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-525.8\">embedding_46: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443486434640&#45;&gt;140443486437104 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443486434640-&gt;140443486437104</title>\n",
       "<path d=\"M131.5,-584.4551C131.5,-576.3828 131.5,-566.6764 131.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-557.5903 131.5,-547.5904 128.0001,-557.5904 135.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443486240840 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443486240840</title>\n",
       "<polygon fill=\"none\" points=\"328,-584.5 328,-620.5 497,-620.5 497,-584.5 328,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-598.8\">input_45: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443486437048 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140443486437048</title>\n",
       "<polygon fill=\"none\" points=\"306.5,-511.5 306.5,-547.5 518.5,-547.5 518.5,-511.5 306.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-525.8\">embedding_47: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443486240840&#45;&gt;140443486437048 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443486240840-&gt;140443486437048</title>\n",
       "<path d=\"M412.5,-584.4551C412.5,-576.3828 412.5,-566.6764 412.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-557.5903 412.5,-547.5904 409.0001,-557.5904 416.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485703024 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443485703024</title>\n",
       "<polygon fill=\"none\" points=\"609,-584.5 609,-620.5 778,-620.5 778,-584.5 609,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-598.8\">input_46: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443485328944 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140443485328944</title>\n",
       "<polygon fill=\"none\" points=\"587.5,-511.5 587.5,-547.5 799.5,-547.5 799.5,-511.5 587.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-525.8\">embedding_48: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443485703024&#45;&gt;140443485328944 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140443485703024-&gt;140443485328944</title>\n",
       "<path d=\"M693.5,-584.4551C693.5,-576.3828 693.5,-566.6764 693.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-557.5903 693.5,-547.5904 690.0001,-557.5904 697.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443621046200 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140443621046200</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-438.5 51.5,-474.5 211.5,-474.5 211.5,-438.5 51.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-452.8\">conv1d_46: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443486437104&#45;&gt;140443621046200 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140443486437104-&gt;140443621046200</title>\n",
       "<path d=\"M131.5,-511.4551C131.5,-503.3828 131.5,-493.6764 131.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-484.5903 131.5,-474.5904 128.0001,-484.5904 135.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443486433576 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140443486433576</title>\n",
       "<polygon fill=\"none\" points=\"332.5,-438.5 332.5,-474.5 492.5,-474.5 492.5,-438.5 332.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-452.8\">conv1d_47: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443486437048&#45;&gt;140443486433576 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140443486437048-&gt;140443486433576</title>\n",
       "<path d=\"M412.5,-511.4551C412.5,-503.3828 412.5,-493.6764 412.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-484.5903 412.5,-474.5904 409.0001,-484.5904 416.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485704088 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140443485704088</title>\n",
       "<polygon fill=\"none\" points=\"613.5,-438.5 613.5,-474.5 773.5,-474.5 773.5,-438.5 613.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-452.8\">conv1d_48: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443485328944&#45;&gt;140443485704088 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140443485328944-&gt;140443485704088</title>\n",
       "<path d=\"M693.5,-511.4551C693.5,-503.3828 693.5,-493.6764 693.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-484.5903 693.5,-474.5904 690.0001,-484.5904 697.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443486436208 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140443486436208</title>\n",
       "<polygon fill=\"none\" points=\"48.5,-365.5 48.5,-401.5 214.5,-401.5 214.5,-365.5 48.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-379.8\">dropout_44: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443621046200&#45;&gt;140443486436208 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140443621046200-&gt;140443486436208</title>\n",
       "<path d=\"M131.5,-438.4551C131.5,-430.3828 131.5,-420.6764 131.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-411.5903 131.5,-401.5904 128.0001,-411.5904 135.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485859008 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140443485859008</title>\n",
       "<polygon fill=\"none\" points=\"329.5,-365.5 329.5,-401.5 495.5,-401.5 495.5,-365.5 329.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-379.8\">dropout_45: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443486433576&#45;&gt;140443485859008 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140443486433576-&gt;140443485859008</title>\n",
       "<path d=\"M412.5,-438.4551C412.5,-430.3828 412.5,-420.6764 412.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-411.5903 412.5,-401.5904 409.0001,-411.5904 416.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485094408 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140443485094408</title>\n",
       "<polygon fill=\"none\" points=\"610.5,-365.5 610.5,-401.5 776.5,-401.5 776.5,-365.5 610.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-379.8\">dropout_46: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443485704088&#45;&gt;140443485094408 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140443485704088-&gt;140443485094408</title>\n",
       "<path d=\"M693.5,-438.4551C693.5,-430.3828 693.5,-420.6764 693.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-411.5903 693.5,-401.5904 690.0001,-411.5904 697.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443486345592 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140443486345592</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 263,-328.5 263,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-306.8\">max_pooling1d_46: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443486436208&#45;&gt;140443486345592 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140443486436208-&gt;140443486345592</title>\n",
       "<path d=\"M131.5,-365.4551C131.5,-357.3828 131.5,-347.6764 131.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-338.5903 131.5,-328.5904 128.0001,-338.5904 135.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485988512 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140443485988512</title>\n",
       "<polygon fill=\"none\" points=\"281,-292.5 281,-328.5 544,-328.5 544,-292.5 281,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-306.8\">max_pooling1d_47: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443485859008&#45;&gt;140443485988512 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140443485859008-&gt;140443485988512</title>\n",
       "<path d=\"M412.5,-365.4551C412.5,-357.3828 412.5,-347.6764 412.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-338.5903 412.5,-328.5904 409.0001,-338.5904 416.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443484826032 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140443484826032</title>\n",
       "<polygon fill=\"none\" points=\"562,-292.5 562,-328.5 825,-328.5 825,-292.5 562,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-306.8\">max_pooling1d_48: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443485094408&#45;&gt;140443484826032 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140443485094408-&gt;140443484826032</title>\n",
       "<path d=\"M693.5,-365.4551C693.5,-357.3828 693.5,-347.6764 693.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-338.5903 693.5,-328.5904 690.0001,-338.5904 697.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443486353896 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140443486353896</title>\n",
       "<polygon fill=\"none\" points=\"113.5,-219.5 113.5,-255.5 263.5,-255.5 263.5,-219.5 113.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-233.8\">flatten_46: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443486345592&#45;&gt;140443486353896 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140443486345592-&gt;140443486353896</title>\n",
       "<path d=\"M145.5899,-292.4551C152.3724,-283.7686 160.6326,-273.1898 168.0877,-263.642\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"170.9789,-265.6263 174.3747,-255.5904 165.4616,-261.3182 170.9789,-265.6263\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485701792 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140443485701792</title>\n",
       "<polygon fill=\"none\" points=\"337.5,-219.5 337.5,-255.5 487.5,-255.5 487.5,-219.5 337.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-233.8\">flatten_47: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443485988512&#45;&gt;140443485701792 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140443485988512-&gt;140443485701792</title>\n",
       "<path d=\"M412.5,-292.4551C412.5,-284.3828 412.5,-274.6764 412.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-265.5903 412.5,-255.5904 409.0001,-265.5904 416.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485096704 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140443485096704</title>\n",
       "<polygon fill=\"none\" points=\"561.5,-219.5 561.5,-255.5 711.5,-255.5 711.5,-219.5 561.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636.5\" y=\"-233.8\">flatten_48: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443484826032&#45;&gt;140443485096704 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140443484826032-&gt;140443485096704</title>\n",
       "<path d=\"M679.4101,-292.4551C672.6276,-283.7686 664.3674,-273.1898 656.9123,-263.642\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"659.5384,-261.3182 650.6253,-255.5904 654.0211,-265.6263 659.5384,-261.3182\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443484928824 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140443484928824</title>\n",
       "<polygon fill=\"none\" points=\"297.5,-146.5 297.5,-182.5 527.5,-182.5 527.5,-146.5 297.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-160.8\">concatenate_13: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140443486353896&#45;&gt;140443484928824 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140443486353896-&gt;140443484928824</title>\n",
       "<path d=\"M243.8708,-219.4551C275.2366,-209.2332 314.6435,-196.3907 347.4739,-185.6916\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"348.5665,-189.0167 356.9899,-182.5904 346.3975,-182.3612 348.5665,-189.0167\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485701792&#45;&gt;140443484928824 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140443485701792-&gt;140443484928824</title>\n",
       "<path d=\"M412.5,-219.4551C412.5,-211.3828 412.5,-201.6764 412.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-192.5903 412.5,-182.5904 409.0001,-192.5904 416.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443485096704&#45;&gt;140443484928824 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140443485096704-&gt;140443484928824</title>\n",
       "<path d=\"M581.1292,-219.4551C549.7634,-209.2332 510.3565,-196.3907 477.5261,-185.6916\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"478.6025,-182.3612 468.0101,-182.5904 476.4335,-189.0167 478.6025,-182.3612\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443484928768 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>140443484928768</title>\n",
       "<polygon fill=\"none\" points=\"344,-73.5 344,-109.5 481,-109.5 481,-73.5 344,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-87.8\">dense_39: Dense</text>\n",
       "</g>\n",
       "<!-- 140443484928824&#45;&gt;140443484928768 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140443484928824-&gt;140443484928768</title>\n",
       "<path d=\"M412.5,-146.4551C412.5,-138.3828 412.5,-128.6764 412.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-119.5903 412.5,-109.5904 409.0001,-119.5904 416.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443417071120 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>140443417071120</title>\n",
       "<polygon fill=\"none\" points=\"344,-.5 344,-36.5 481,-36.5 481,-.5 344,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-14.8\">dense_40: Dense</text>\n",
       "</g>\n",
       "<!-- 140443484928768&#45;&gt;140443417071120 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140443484928768-&gt;140443417071120</title>\n",
       "<path d=\"M412.5,-73.4551C412.5,-65.3828 412.5,-55.6764 412.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-46.5903 412.5,-36.5904 409.0001,-46.5904 416.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(ncnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1800/1800 [==============================] - 42s 24ms/step - loss: 0.6921 - acc: 0.5222\n",
      "Epoch 2/8\n",
      "1800/1800 [==============================] - 40s 22ms/step - loss: 0.5597 - acc: 0.7372\n",
      "Epoch 3/8\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0993 - acc: 0.9689\n",
      "Epoch 4/8\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0072 - acc: 0.9994\n",
      "Epoch 5/8\n",
      "1800/1800 [==============================] - 37s 21ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 6/8\n",
      "1800/1800 [==============================] - 40s 22ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 7/8\n",
      "1800/1800 [==============================] - 39s 21ms/step - loss: 6.8773e-04 - acc: 1.0000\n",
      "Epoch 8/8\n",
      "1800/1800 [==============================] - 41s 23ms/step - loss: 4.9651e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb87e5bc50>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncnn_model.fit( [ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain] , ytrain, epochs=8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_model.save('ncnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtrain=np.array(ncnn_Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 10s 5ms/step\n",
      "Train Accuracy: 100.00\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Test Accuracy: 87.00\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = ncnn_model.evaluate([ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain], ytrain, verbose=1)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = ncnn_model.evaluate([ncnn_Xtest,ncnn_Xtest,ncnn_Xtest], ytest, verbose=1)\n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ncnn_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict([padded,padded,padded], verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.5051639\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: POSITIVE (50.516%) \n",
      "\n",
      "loved would recommend movie loved movie goodness goodness goodness would recommend movie everyone\n",
      "0.6596375\n",
      "Review: [I loved This Movie.I would recommend this movie to everyone.I loved This Movie. GoodNess Goodness Goodness I would recommend this movie to everyone.]\n",
      "Sentiment: POSITIVE (65.964%) \n",
      "\n",
      "bad bad bad hate movie one waste money watching\n",
      "0.18181247\n",
      "Review: [Bad , Bad , Bad I Hate this Movie No One Should waste money watching this]\n",
      "Sentiment: NEGATIVE (81.819%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie.I would recommend this movie to everyone.I loved This Movie. GoodNess Goodness Goodness I would recommend this movie to everyone.'\n",
    "percent, sentiment =  predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Bad , Bad , Bad I Hate this Movie No One Should waste money watching this'\n",
    "percent, sentiment =  predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "fn\n",
    "nf=sorted(os.listdir('txt_sentoken/mtest/neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested using MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Testing For---MLP-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['pos_file0.txt', 'pos_file1.txt', 'pos_file2.txt', 'pos_file3.txt', 'pos_file4.txt', 'pos_file5.txt', 'pos_file6.txt', 'pos_file7.txt', 'pos_file8.txt', 'pos_file9.txt']\n",
      "txt_sentoken/mtest/pos/pos_file0.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "Sentiment: NEGATIVE (68.476%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "Sentiment: POSITIVE (63.445%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file2.txt\n",
      "awesome movie innocent man represents world hes loyal humble honest would never cheat lie everything good heart reaches success important tasks life america could live interesting life meet elvis president kennedy john goes war everything following heart becomes american hero gary sinise great american actor best performance served vietnam forrest also hero protecting men fighting country mustsee movie everyone enjoy american culture icons well learn american history useful\n",
      "Sentiment: POSITIVE (79.492%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file3.txt\n",
      "great really great movie threw us deep end fishes great ride kept attention start finish long saga cgi gone mad really great movie delivered exactly tin jason gets special mention gave solid endearing performance line humour superhero loved\n",
      "Sentiment: POSITIVE (63.620%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file4.txt\n",
      "amazing film delivered epic adventure story stunning visual effects action awesome superhero film delivers epic adventure scale action sequences visual effects great story consider best dc movie beating wonder woman character seems like cheesy character even adapted onto big screen jason performance character makes character even achieves golden outfit plot great adventure story rushed sub plots follows nicole kidman queen atlantis rescued thomas morrison fall love son named arthur jason soon become decades fly found amber heard informs step brother patrick wilson trying wage war surface people trying unite seven underwater create havoc surface people willem dafoe help needing find true atlantis order claim rightful place throne fight army hired pirates led black ii plot delivered epic scale adventure travelling atlantis looked like beautiful colorful city underwater astonishing film action sequences big well directed lot action throughout fighting pirates submarine epic sub water fight scene fighting atlantis soldiers opening scene one long camera take well choreographed james wan direction camera around action sequences worked well known small budget horror movies like saw action style furious seven film awesome delivered great underwater chase scene almost felt like legacy beautiful lighting vehicles atlantis lighting chase scene italy facing black soldiers thrilling climax battle sequence breathtaking ocean master fighting best choreographed fight scene cinematography work visual effects breathtaking action underwater land great epic scale cinematography work well done captured action choreography music score rupert great good score made movie feel like adventure also soundtrack songs great fun jason great amber heard beautiful great willem dafoe patrick wilson ii nicole kidman great characters james direction brilliant making adventure story epic action overall great film say best dc movie definitely recommend seeing big screen theatres looking adventure story exciting epic action sequences beautiful stunning visual effects film worth seeing\n",
      "Sentiment: POSITIVE (71.744%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file5.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "Sentiment: POSITIVE (58.331%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "Sentiment: NEGATIVE (57.845%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file7.txt\n",
      "film first expect delight film behind ease irony hides deep meaning film watch without stopping although far blockbuster waiting years peter green book plot tells journey southern states talented black pianist ali driver hired solely tour viggo minor first sight little hero mortensen journey becomes pianist true friend support even treasure useful knowledge life society draws boss useful skills love art good plot simple film looks incredibly easy dialogues written ironically absolutely boring time remain filled meaning viewer relationship characters main motive film could different people respects able become friends life weeks noted racism time still extremely common america especially south incredibly subtly written hotels colored individual even nuances time film shown perfectly feel ashamed existed literally wanted stand give characters face several scenes surprisingly director peter previously shot mostly comedies mostly brother first film comedy drama genre point view handled perfectly would love see else would film later although cant say set apart interesting director perhaps fact film made peter strongly contributed nobility picture terms humor funny fact one screenwriters film nick father main character frank tony script written basis records made nick later records viggo mortensen learns speak italian hero italian film shot spirit beautiful cars time well shown art costume large houses rich public gone wind immediately remembered film simple kind sincere without strain without life horrors know someone dies someone killed someone prison atmosphere film soft light literally find time terrific cars colors rainbow men flawless day costumes colors gentlemen whiskey bar hotels listening brilliant pianist green book picture slavery racism humiliation although touches topics green book film honesty human dignity true friendship mutual support people film fact difficult change thinking society alone even brilliant famous\n",
      "Sentiment: POSITIVE (66.034%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file8.txt\n",
      "beautiful emotional harry movie experience richard playing reallife straight decides travel old riding john iowa see ailing brother harry dean stanton drivers license doesnt like public transportation one else drive two brothers speaking terms many years clock literally ticking one last chance see one another hopefully make past mistakes sissy gives one finest performances slightly mentally daughter supporting players real heartfelt caricatures americas outstanding filmmaker david lynch finest living american director along martin scorsese goes totally turn triumphs like elephant man blue velvet twin peaks fire walk mulholland dr quietly creates family film deep messages people ages backgrounds paints picture america old ways life still important total revelation excruciating pain throughout filming due terminal cancer terrible notice stands little movie almost always filmed waist sadly would final performance committed suicide shortly oscar nod become oldest nominee ever best actor category really become trivial time passes role thing shines brightly forever truly legacy production involved stars\n",
      "Sentiment: POSITIVE (64.701%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file9.txt\n",
      "simply best better rest january perfect cast couldnt put together movie amber heard beautiful creature land sea even orange hair wet hair stunning action going around character distress either happy finally worldwide audience legendary beauty jason build hybrid looking eyes comic book creators could else could suited role something little tongue cheek personality helps talented patrick wilson evil brother shows versatile jumping wan movies like superhero movie effortlessly nicole towering height finally put good use queen atlantis visuals also beautiful beyond expectations much sea interesting surface action satisfying epic story remains easy follow start finish marvel movies could learn dc standard future action hero movies\n",
      "Sentiment: POSITIVE (51.487%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['neg_file0.txt', 'neg_file1.txt', 'neg_file2.txt', 'neg_file3.txt', 'neg_file4.txt', 'neg_file5.txt', 'neg_file6.txt', 'neg_file7.txt', 'neg_file8.txt', 'neg_file9.txt']\n",
      "txt_sentoken/mtest/neg/neg_file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "Sentiment: NEGATIVE (69.185%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "Sentiment: NEGATIVE (85.516%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "Sentiment: NEGATIVE (79.956%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "Sentiment: NEGATIVE (84.983%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "Sentiment: NEGATIVE (81.302%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "Sentiment: NEGATIVE (63.747%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "Sentiment: NEGATIVE (71.487%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "Sentiment: NEGATIVE (71.487%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "Sentiment: NEGATIVE (57.982%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "Sentiment: NEGATIVE (67.620%) \n",
      "\n",
      "---------Positive List----------\n",
      "  Sentiment    Percent\n",
      "0  NEGATIVE  68.475601\n",
      "1  POSITIVE  63.445067\n",
      "2  POSITIVE  79.491866\n",
      "3  POSITIVE  63.619858\n",
      "4  POSITIVE  71.743715\n",
      "5  POSITIVE  58.330935\n",
      "6  NEGATIVE  57.844588\n",
      "7  POSITIVE  66.033560\n",
      "8  POSITIVE  64.700735\n",
      "9  POSITIVE  51.486754\n",
      "---------Negative List----------\n",
      "  Sentiment    Percent\n",
      "0  NEGATIVE  69.185138\n",
      "1  NEGATIVE  85.515815\n",
      "2  NEGATIVE  79.956494\n",
      "3  NEGATIVE  84.983420\n",
      "4  NEGATIVE  81.301878\n",
      "5  NEGATIVE  63.747370\n",
      "6  NEGATIVE  71.487355\n",
      "7  NEGATIVE  71.487355\n",
      "8  NEGATIVE  57.981867\n",
      "9  NEGATIVE  67.619750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderMLP(vocab,tokenizer,model):\n",
    "    print(\"-------------Testing For---MLP-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([sentiment,percent*100])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([sentiment,percent*100])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list, columns=['Sentiment','Percent'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['Sentiment','Percent'])\n",
    "    print(ndf.head(n=10))\n",
    "    return pdf , ndf\n",
    "    \n",
    "mlppdf,mlpndf=testFromFolderMLP(vocab,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlppdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlpndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested Using CNN 1 Layer Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing For---CNN 1 Layer-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['pos_file0.txt', 'pos_file1.txt', 'pos_file2.txt', 'pos_file3.txt', 'pos_file4.txt', 'pos_file5.txt', 'pos_file6.txt', 'pos_file7.txt', 'pos_file8.txt', 'pos_file9.txt']\n",
      "-----------Processing ----------pos_file0.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "0.39873114\n",
      "Sentiment: NEGATIVE (60.127%) \n",
      "\n",
      "-----------Processing ----------pos_file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "0.89265907\n",
      "Sentiment: POSITIVE (89.266%) \n",
      "\n",
      "-----------Processing ----------pos_file2.txt\n",
      "awesome movie innocent man represents world hes loyal humble honest would never cheat lie everything good heart reaches success important tasks life america could live interesting life meet elvis president kennedy john goes war everything following heart becomes american hero gary sinise great american actor best performance served vietnam forrest also hero protecting men fighting country mustsee movie everyone enjoy american culture icons well learn american history useful\n",
      "0.9990312\n",
      "Sentiment: POSITIVE (99.903%) \n",
      "\n",
      "-----------Processing ----------pos_file3.txt\n",
      "great really great movie threw us deep end fishes great ride kept attention start finish long saga cgi gone mad really great movie delivered exactly tin jason gets special mention gave solid endearing performance line humour superhero loved\n",
      "0.84135956\n",
      "Sentiment: POSITIVE (84.136%) \n",
      "\n",
      "-----------Processing ----------pos_file4.txt\n",
      "amazing film delivered epic adventure story stunning visual effects action awesome superhero film delivers epic adventure scale action sequences visual effects great story consider best dc movie beating wonder woman character seems like cheesy character even adapted onto big screen jason performance character makes character even achieves golden outfit plot great adventure story rushed sub plots follows nicole kidman queen atlantis rescued thomas morrison fall love son named arthur jason soon become decades fly found amber heard informs step brother patrick wilson trying wage war surface people trying unite seven underwater create havoc surface people willem dafoe help needing find true atlantis order claim rightful place throne fight army hired pirates led black ii plot delivered epic scale adventure travelling atlantis looked like beautiful colorful city underwater astonishing film action sequences big well directed lot action throughout fighting pirates submarine epic sub water fight scene fighting atlantis soldiers opening scene one long camera take well choreographed james wan direction camera around action sequences worked well known small budget horror movies like saw action style furious seven film awesome delivered great underwater chase scene almost felt like legacy beautiful lighting vehicles atlantis lighting chase scene italy facing black soldiers thrilling climax battle sequence breathtaking ocean master fighting best choreographed fight scene cinematography work visual effects breathtaking action underwater land great epic scale cinematography work well done captured action choreography music score rupert great good score made movie feel like adventure also soundtrack songs great fun jason great amber heard beautiful great willem dafoe patrick wilson ii nicole kidman great characters james direction brilliant making adventure story epic action overall great film say best dc movie definitely recommend seeing big screen theatres looking adventure story exciting epic action sequences beautiful stunning visual effects film worth seeing\n",
      "1.0\n",
      "Sentiment: POSITIVE (100.000%) \n",
      "\n",
      "-----------Processing ----------pos_file5.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "0.99995303\n",
      "Sentiment: POSITIVE (99.995%) \n",
      "\n",
      "-----------Processing ----------pos_file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "0.13423839\n",
      "Sentiment: NEGATIVE (86.576%) \n",
      "\n",
      "-----------Processing ----------pos_file7.txt\n",
      "film first expect delight film behind ease irony hides deep meaning film watch without stopping although far blockbuster waiting years peter green book plot tells journey southern states talented black pianist ali driver hired solely tour viggo minor first sight little hero mortensen journey becomes pianist true friend support even treasure useful knowledge life society draws boss useful skills love art good plot simple film looks incredibly easy dialogues written ironically absolutely boring time remain filled meaning viewer relationship characters main motive film could different people respects able become friends life weeks noted racism time still extremely common america especially south incredibly subtly written hotels colored individual even nuances time film shown perfectly feel ashamed existed literally wanted stand give characters face several scenes surprisingly director peter previously shot mostly comedies mostly brother first film comedy drama genre point view handled perfectly would love see else would film later although cant say set apart interesting director perhaps fact film made peter strongly contributed nobility picture terms humor funny fact one screenwriters film nick father main character frank tony script written basis records made nick later records viggo mortensen learns speak italian hero italian film shot spirit beautiful cars time well shown art costume large houses rich public gone wind immediately remembered film simple kind sincere without strain without life horrors know someone dies someone killed someone prison atmosphere film soft light literally find time terrific cars colors rainbow men flawless day costumes colors gentlemen whiskey bar hotels listening brilliant pianist green book picture slavery racism humiliation although touches topics green book film honesty human dignity true friendship mutual support people film fact difficult change thinking society alone even brilliant famous\n",
      "0.9963355\n",
      "Sentiment: POSITIVE (99.634%) \n",
      "\n",
      "-----------Processing ----------pos_file8.txt\n",
      "beautiful emotional harry movie experience richard playing reallife straight decides travel old riding john iowa see ailing brother harry dean stanton drivers license doesnt like public transportation one else drive two brothers speaking terms many years clock literally ticking one last chance see one another hopefully make past mistakes sissy gives one finest performances slightly mentally daughter supporting players real heartfelt caricatures americas outstanding filmmaker david lynch finest living american director along martin scorsese goes totally turn triumphs like elephant man blue velvet twin peaks fire walk mulholland dr quietly creates family film deep messages people ages backgrounds paints picture america old ways life still important total revelation excruciating pain throughout filming due terminal cancer terrible notice stands little movie almost always filmed waist sadly would final performance committed suicide shortly oscar nod become oldest nominee ever best actor category really become trivial time passes role thing shines brightly forever truly legacy production involved stars\n",
      "0.9813967\n",
      "Sentiment: POSITIVE (98.140%) \n",
      "\n",
      "-----------Processing ----------pos_file9.txt\n",
      "simply best better rest january perfect cast couldnt put together movie amber heard beautiful creature land sea even orange hair wet hair stunning action going around character distress either happy finally worldwide audience legendary beauty jason build hybrid looking eyes comic book creators could else could suited role something little tongue cheek personality helps talented patrick wilson evil brother shows versatile jumping wan movies like superhero movie effortlessly nicole towering height finally put good use queen atlantis visuals also beautiful beyond expectations much sea interesting surface action satisfying epic story remains easy follow start finish marvel movies could learn dc standard future action hero movies\n",
      "0.57196635\n",
      "Sentiment: POSITIVE (57.197%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['neg_file0.txt', 'neg_file1.txt', 'neg_file2.txt', 'neg_file3.txt', 'neg_file4.txt', 'neg_file5.txt', 'neg_file6.txt', 'neg_file7.txt', 'neg_file8.txt', 'neg_file9.txt']\n",
      "txt_sentoken/mtest/neg/neg_file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "0.0003921597\n",
      "Sentiment: NEGATIVE (99.961%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "0.023352487\n",
      "Sentiment: NEGATIVE (97.665%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "0.033625938\n",
      "Sentiment: NEGATIVE (96.637%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "0.35020414\n",
      "Sentiment: NEGATIVE (64.980%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "0.004413831\n",
      "Sentiment: NEGATIVE (99.559%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "0.40504828\n",
      "Sentiment: NEGATIVE (59.495%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.04266677\n",
      "Sentiment: NEGATIVE (95.733%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.04266677\n",
      "Sentiment: NEGATIVE (95.733%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "0.42865244\n",
      "Sentiment: NEGATIVE (57.135%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "0.030771319\n",
      "Sentiment: NEGATIVE (96.923%) \n",
      "\n",
      "---------Positive List----------\n",
      "   Analysis     Percent\n",
      "0  NEGATIVE   60.126886\n",
      "1  POSITIVE   89.265907\n",
      "2  POSITIVE   99.903119\n",
      "3  POSITIVE   84.135956\n",
      "4  POSITIVE  100.000000\n",
      "5  POSITIVE   99.995303\n",
      "6  NEGATIVE   86.576161\n",
      "7  POSITIVE   99.633551\n",
      "8  POSITIVE   98.139668\n",
      "9  POSITIVE   57.196635\n",
      "---------Negative List----------\n",
      "  File Name    Percent\n",
      "0  NEGATIVE  99.960784\n",
      "1  NEGATIVE  97.664751\n",
      "2  NEGATIVE  96.637406\n",
      "3  NEGATIVE  64.979586\n",
      "4  NEGATIVE  99.558617\n",
      "5  NEGATIVE  59.495172\n",
      "6  NEGATIVE  95.733323\n",
      "7  NEGATIVE  95.733323\n",
      "8  NEGATIVE  57.134756\n",
      "9  NEGATIVE  96.922868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderCnn(vocab,tokenizer,l,model):\n",
    "    print(\"------------Testing For---CNN 1 Layer-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(\"-----------Processing ----------%s\"%files)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_cnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([sentiment,percent*100])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_cnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([sentiment,percent*100])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list,columns=['Analysis','Percent'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['File Name','Percent'])\n",
    "    print(ndf.head(n=10))\n",
    "    \n",
    "    return pdf,ndf\n",
    "    \n",
    "cnnpdf,cnnndf=testFromFolderCnn(vocab,cnn_tokenizer,max_length,embed_cnn_model)\n",
    "\n",
    "\n",
    "# percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested Using CNN Multichannel Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing For---CNN 1 Layer-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['pos_file0.txt', 'pos_file1.txt', 'pos_file2.txt', 'pos_file3.txt', 'pos_file4.txt', 'pos_file5.txt', 'pos_file6.txt', 'pos_file7.txt', 'pos_file8.txt', 'pos_file9.txt']\n",
      "txt_sentoken/mtest/pos/pos_file0.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "0.28563103\n",
      "Sentiment: NEGATIVE (71.437%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "0.5723072\n",
      "Sentiment: POSITIVE (57.231%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file2.txt\n",
      "awesome movie innocent man represents world hes loyal humble honest would never cheat lie everything good heart reaches success important tasks life america could live interesting life meet elvis president kennedy john goes war everything following heart becomes american hero gary sinise great american actor best performance served vietnam forrest also hero protecting men fighting country mustsee movie everyone enjoy american culture icons well learn american history useful\n",
      "0.93889457\n",
      "Sentiment: POSITIVE (93.889%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file3.txt\n",
      "great really great movie threw us deep end fishes great ride kept attention start finish long saga cgi gone mad really great movie delivered exactly tin jason gets special mention gave solid endearing performance line humour superhero loved\n",
      "0.37369558\n",
      "Sentiment: NEGATIVE (62.630%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file4.txt\n",
      "amazing film delivered epic adventure story stunning visual effects action awesome superhero film delivers epic adventure scale action sequences visual effects great story consider best dc movie beating wonder woman character seems like cheesy character even adapted onto big screen jason performance character makes character even achieves golden outfit plot great adventure story rushed sub plots follows nicole kidman queen atlantis rescued thomas morrison fall love son named arthur jason soon become decades fly found amber heard informs step brother patrick wilson trying wage war surface people trying unite seven underwater create havoc surface people willem dafoe help needing find true atlantis order claim rightful place throne fight army hired pirates led black ii plot delivered epic scale adventure travelling atlantis looked like beautiful colorful city underwater astonishing film action sequences big well directed lot action throughout fighting pirates submarine epic sub water fight scene fighting atlantis soldiers opening scene one long camera take well choreographed james wan direction camera around action sequences worked well known small budget horror movies like saw action style furious seven film awesome delivered great underwater chase scene almost felt like legacy beautiful lighting vehicles atlantis lighting chase scene italy facing black soldiers thrilling climax battle sequence breathtaking ocean master fighting best choreographed fight scene cinematography work visual effects breathtaking action underwater land great epic scale cinematography work well done captured action choreography music score rupert great good score made movie feel like adventure also soundtrack songs great fun jason great amber heard beautiful great willem dafoe patrick wilson ii nicole kidman great characters james direction brilliant making adventure story epic action overall great film say best dc movie definitely recommend seeing big screen theatres looking adventure story exciting epic action sequences beautiful stunning visual effects film worth seeing\n",
      "0.99960214\n",
      "Sentiment: POSITIVE (99.960%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file5.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "0.9507237\n",
      "Sentiment: POSITIVE (95.072%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "0.37134758\n",
      "Sentiment: NEGATIVE (62.865%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file7.txt\n",
      "film first expect delight film behind ease irony hides deep meaning film watch without stopping although far blockbuster waiting years peter green book plot tells journey southern states talented black pianist ali driver hired solely tour viggo minor first sight little hero mortensen journey becomes pianist true friend support even treasure useful knowledge life society draws boss useful skills love art good plot simple film looks incredibly easy dialogues written ironically absolutely boring time remain filled meaning viewer relationship characters main motive film could different people respects able become friends life weeks noted racism time still extremely common america especially south incredibly subtly written hotels colored individual even nuances time film shown perfectly feel ashamed existed literally wanted stand give characters face several scenes surprisingly director peter previously shot mostly comedies mostly brother first film comedy drama genre point view handled perfectly would love see else would film later although cant say set apart interesting director perhaps fact film made peter strongly contributed nobility picture terms humor funny fact one screenwriters film nick father main character frank tony script written basis records made nick later records viggo mortensen learns speak italian hero italian film shot spirit beautiful cars time well shown art costume large houses rich public gone wind immediately remembered film simple kind sincere without strain without life horrors know someone dies someone killed someone prison atmosphere film soft light literally find time terrific cars colors rainbow men flawless day costumes colors gentlemen whiskey bar hotels listening brilliant pianist green book picture slavery racism humiliation although touches topics green book film honesty human dignity true friendship mutual support people film fact difficult change thinking society alone even brilliant famous\n",
      "0.9564443\n",
      "Sentiment: POSITIVE (95.644%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file8.txt\n",
      "beautiful emotional harry movie experience richard playing reallife straight decides travel old riding john iowa see ailing brother harry dean stanton drivers license doesnt like public transportation one else drive two brothers speaking terms many years clock literally ticking one last chance see one another hopefully make past mistakes sissy gives one finest performances slightly mentally daughter supporting players real heartfelt caricatures americas outstanding filmmaker david lynch finest living american director along martin scorsese goes totally turn triumphs like elephant man blue velvet twin peaks fire walk mulholland dr quietly creates family film deep messages people ages backgrounds paints picture america old ways life still important total revelation excruciating pain throughout filming due terminal cancer terrible notice stands little movie almost always filmed waist sadly would final performance committed suicide shortly oscar nod become oldest nominee ever best actor category really become trivial time passes role thing shines brightly forever truly legacy production involved stars\n",
      "0.93685037\n",
      "Sentiment: POSITIVE (93.685%) \n",
      "\n",
      "txt_sentoken/mtest/pos/pos_file9.txt\n",
      "simply best better rest january perfect cast couldnt put together movie amber heard beautiful creature land sea even orange hair wet hair stunning action going around character distress either happy finally worldwide audience legendary beauty jason build hybrid looking eyes comic book creators could else could suited role something little tongue cheek personality helps talented patrick wilson evil brother shows versatile jumping wan movies like superhero movie effortlessly nicole towering height finally put good use queen atlantis visuals also beautiful beyond expectations much sea interesting surface action satisfying epic story remains easy follow start finish marvel movies could learn dc standard future action hero movies\n",
      "0.5679325\n",
      "Sentiment: POSITIVE (56.793%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['neg_file0.txt', 'neg_file1.txt', 'neg_file2.txt', 'neg_file3.txt', 'neg_file4.txt', 'neg_file5.txt', 'neg_file6.txt', 'neg_file7.txt', 'neg_file8.txt', 'neg_file9.txt']\n",
      "txt_sentoken/mtest/neg/neg_file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "0.025218584\n",
      "Sentiment: NEGATIVE (97.478%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "0.03821642\n",
      "Sentiment: NEGATIVE (96.178%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "0.09719355\n",
      "Sentiment: NEGATIVE (90.281%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "0.29838014\n",
      "Sentiment: NEGATIVE (70.162%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "0.010015873\n",
      "Sentiment: NEGATIVE (98.998%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "0.3176915\n",
      "Sentiment: NEGATIVE (68.231%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.1849973\n",
      "Sentiment: NEGATIVE (81.500%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1849973\n",
      "Sentiment: NEGATIVE (81.500%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "0.4026397\n",
      "Sentiment: NEGATIVE (59.736%) \n",
      "\n",
      "txt_sentoken/mtest/neg/neg_file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "0.13254745\n",
      "Sentiment: NEGATIVE (86.745%) \n",
      "\n",
      "---------Positive List----------\n",
      "  File Name   Analysis\n",
      "0  NEGATIVE  71.436897\n",
      "1  POSITIVE  57.230723\n",
      "2  POSITIVE  93.889457\n",
      "3  NEGATIVE  62.630442\n",
      "4  POSITIVE  99.960214\n",
      "5  POSITIVE  95.072371\n",
      "6  NEGATIVE  62.865242\n",
      "7  POSITIVE  95.644432\n",
      "8  POSITIVE  93.685037\n",
      "9  POSITIVE  56.793249\n",
      "---------Negative List----------\n",
      "  File Name   Analysis\n",
      "0  NEGATIVE  97.478142\n",
      "1  NEGATIVE  96.178358\n",
      "2  NEGATIVE  90.280645\n",
      "3  NEGATIVE  70.161986\n",
      "4  NEGATIVE  98.998413\n",
      "5  NEGATIVE  68.230850\n",
      "6  NEGATIVE  81.500269\n",
      "7  NEGATIVE  81.500269\n",
      "8  NEGATIVE  59.736031\n",
      "9  NEGATIVE  86.745255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderNCnn(vocab,tokenizer,l,model):\n",
    "    print(\"------------Testing For---CNN 1 Layer-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_ncnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([sentiment,percent*100])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_ncnn_sentiment(text, vocab, tokenizer, l,model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([sentiment,percent*100])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list,columns=['File Name','Analysis'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['File Name','Analysis'])\n",
    "    print(ndf.head(n=10))\n",
    "    return pdf,ndf\n",
    "    \n",
    "ncnnpdf,ncnnndf=testFromFolderNCnn(vocab,ncnn_tokenizer,length,ncnn_model)\n",
    "\n",
    "\n",
    "# text = 'It is an ok movie one time watch'\n",
    "# percent, sentiment = predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnnpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnnndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------Evaluating For MLP Model----------------\n",
      "\n",
      "200/200 [==============================] - 0s 264us/step\n",
      "Test Accuracy: 86.000000\n",
      "\n",
      "---Evaluating For Embedding + 1DCNN Model--------\n",
      "\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "Test Loss: 49.719241\n",
      "Test Accuracy: 86.500000\n",
      "\n",
      "---Evaluating For N-gram CNN Model--------\n",
      "\n",
      "200/200 [==============================] - 2s 8ms/step\n",
      "Test Accuracy: 87.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---------Evaluating For MLP Model----------------\\n\")\n",
    "loss, acc  = model.evaluate(Xtest, ytest, verbose=1)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "print(\"\\n---Evaluating For Embedding + 1DCNN Model--------\\n\")\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtest, ytest, verbose=1)\n",
    "print('Test Loss: %f' % (cnn_loss*100))\n",
    "print('Test Accuracy: %f' % (cnn_acc*100))\n",
    "\n",
    "print(\"\\n---Evaluating For N-gram CNN Model--------\\n\")\n",
    "ncnn_loss, ncnn_acc = ncnn_model.evaluate([ncnn_Xtest,ncnn_Xtest,ncnn_Xtest], ytest, verbose=1)\n",
    "print('Test Accuracy: %.2f' % (ncnn_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>FILE_NAME</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLP</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CNN</th>\n",
       "      <th colspan=\"2\" halign=\"left\">NCNN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos_file0.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>68.4756</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>60.1269</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>71.4369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos_file1.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>63.4451</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>89.2659</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>57.2307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos_file2.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>79.4919</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>99.9031</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>93.8895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos_file3.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>63.6199</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>84.136</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>62.6304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos_file4.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>71.7437</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>100</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>99.9602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pos_file5.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>58.3309</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>99.9953</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>95.0724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos_file6.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>57.8446</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>86.5762</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>62.8652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pos_file7.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>66.0336</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>99.6336</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>95.6444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos_file8.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>64.7007</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>98.1397</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>93.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pos_file9.txt</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>51.4868</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>57.1966</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>56.7932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FILE_NAME       MLP                CNN               NCNN         \n",
       "                 SENTIMENT  PERCENT SENTIMENT  PERCENT SENTIMENT  PERCENT\n",
       "0  pos_file0.txt  NEGATIVE  68.4756  NEGATIVE  60.1269  NEGATIVE  71.4369\n",
       "1  pos_file1.txt  POSITIVE  63.4451  POSITIVE  89.2659  POSITIVE  57.2307\n",
       "2  pos_file2.txt  POSITIVE  79.4919  POSITIVE  99.9031  POSITIVE  93.8895\n",
       "3  pos_file3.txt  POSITIVE  63.6199  POSITIVE   84.136  NEGATIVE  62.6304\n",
       "4  pos_file4.txt  POSITIVE  71.7437  POSITIVE      100  POSITIVE  99.9602\n",
       "5  pos_file5.txt  POSITIVE  58.3309  POSITIVE  99.9953  POSITIVE  95.0724\n",
       "6  pos_file6.txt  NEGATIVE  57.8446  NEGATIVE  86.5762  NEGATIVE  62.8652\n",
       "7  pos_file7.txt  POSITIVE  66.0336  POSITIVE  99.6336  POSITIVE  95.6444\n",
       "8  pos_file8.txt  POSITIVE  64.7007  POSITIVE  98.1397  POSITIVE   93.685\n",
       "9  pos_file9.txt  POSITIVE  51.4868  POSITIVE  57.1966  POSITIVE  56.7932"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(fn)\n",
    "# print(mlppdf)\n",
    "# print(cnnpdf)\n",
    "# print(ncnnpdf)\n",
    "# #pd.concat([mlppdf, cnnpdf], axis=1)\n",
    "# file_df = pd.DataFrame({'filename':fn})\n",
    "col = pd.MultiIndex.from_product([['MLP', 'CNN', 'NCNN'], ['SENTIMENT', 'PERCENT']])\n",
    "df = pd.DataFrame(columns=col)\n",
    "#df.A = mlppdf\n",
    "#df['file_name'] = fn\n",
    "df.insert(loc=0, column='FILE_NAME', value=fn)\n",
    "df.MLP = mlppdf.values\n",
    "df.CNN = cnnpdf.values\n",
    "df.NCNN = ncnnpdf.values\n",
    "#file_df.concat(df, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>FILE_NAME</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLP</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CNN</th>\n",
       "      <th colspan=\"2\" halign=\"left\">NCNN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>PERCENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg_file0.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>69.1851</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>99.9608</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>97.4781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg_file1.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>85.5158</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>97.6648</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>96.1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg_file2.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>79.9565</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>96.6374</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>90.2806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg_file3.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>84.9834</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>64.9796</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>70.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg_file4.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>81.3019</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>99.5586</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>98.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neg_file5.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>63.7474</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>59.4952</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>68.2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neg_file6.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>71.4874</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>95.7333</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>81.5003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neg_file7.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>71.4874</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>95.7333</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>81.5003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neg_file8.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>57.9819</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>57.1348</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>59.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neg_file9.txt</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>67.6197</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>96.9229</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>86.7453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FILE_NAME       MLP                CNN               NCNN         \n",
       "                 SENTIMENT  PERCENT SENTIMENT  PERCENT SENTIMENT  PERCENT\n",
       "0  neg_file0.txt  NEGATIVE  69.1851  NEGATIVE  99.9608  NEGATIVE  97.4781\n",
       "1  neg_file1.txt  NEGATIVE  85.5158  NEGATIVE  97.6648  NEGATIVE  96.1784\n",
       "2  neg_file2.txt  NEGATIVE  79.9565  NEGATIVE  96.6374  NEGATIVE  90.2806\n",
       "3  neg_file3.txt  NEGATIVE  84.9834  NEGATIVE  64.9796  NEGATIVE   70.162\n",
       "4  neg_file4.txt  NEGATIVE  81.3019  NEGATIVE  99.5586  NEGATIVE  98.9984\n",
       "5  neg_file5.txt  NEGATIVE  63.7474  NEGATIVE  59.4952  NEGATIVE  68.2308\n",
       "6  neg_file6.txt  NEGATIVE  71.4874  NEGATIVE  95.7333  NEGATIVE  81.5003\n",
       "7  neg_file7.txt  NEGATIVE  71.4874  NEGATIVE  95.7333  NEGATIVE  81.5003\n",
       "8  neg_file8.txt  NEGATIVE  57.9819  NEGATIVE  57.1348  NEGATIVE   59.736\n",
       "9  neg_file9.txt  NEGATIVE  67.6197  NEGATIVE  96.9229  NEGATIVE  86.7453"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncol = pd.MultiIndex.from_product([['MLP', 'CNN', 'NCNN'], ['SENTIMENT', 'PERCENT']])\n",
    "ndf = pd.DataFrame(columns=col)\n",
    "#df.A = mlppdf\n",
    "#df['file_name'] = fn\n",
    "ndf.insert(loc=0, column='FILE_NAME', value=nf)\n",
    "ndf.MLP = mlpndf.values\n",
    "ndf.CNN = cnnndf.values\n",
    "ndf.NCNN = ncnnndf.values\n",
    "#file_df.concat(df, axis=1)\n",
    "ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Word Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes= ['binary', 'count', 'tfidf', 'freq']\n",
    "# #Prepare Bag - Of - Words Encoding\n",
    "# results=pd.DataFrame()\n",
    "# def prepare_data(train,test,m):\n",
    "#     token=Tokenizer()\n",
    "#     token.fit_on_texts(train)\n",
    "#     Xtrain=token.texts_to_matrix(train, mode=mode)\n",
    "#     Xtest = token.texts_to_matrix(test, mode=mode)\n",
    "#     return Xtrain,Xtest\n",
    "\n",
    "# #Evaluate\n",
    "# def evaluate_mode(Xtrain,ytrain,Xtest,ytest):\n",
    "#     scores=list()\n",
    "#     no_r=10\n",
    "#     for _ in range(no_r):\n",
    "#         model=Sequential() \n",
    "#         model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "#         model.add(Dense(1,activation='sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#         model.fit(Xtrain,ytrain,epochs=10,verbose=1)\n",
    "#         loss,acc=model.evaluate(Xtest,ytest,verbose=1)\n",
    "#         scores.append(acc)\n",
    "#         print(\"%d accuracy : %s \"%((_+1),acc))\n",
    "#     return scores\n",
    "\n",
    "# for mode in modes:\n",
    "#     Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "#     results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.describe())\n",
    "# plt.figure(figsize=(16,6))\n",
    "# results.boxplot()\n",
    "# plt.xlabel(\"Types\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
