{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Of The Important Libraries Required for Solving This Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import graphviz\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.png',\n",
       " 'ncnn_model.h5',\n",
       " '.ipynb_checkpoints',\n",
       " 'negative.txt',\n",
       " 'mlp.ipynb',\n",
       " 'Sentiment_Analysis_Udacity.ipynb',\n",
       " 'txt_sentoken',\n",
       " 'vocab.txt',\n",
       " 'rp.tar.gz',\n",
       " 'review_polarity',\n",
       " 'positive.txt',\n",
       " 'cnn_model.png',\n",
       " 'cnn']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step \n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load document into the notebook\n",
    "def load_document(fileName):\n",
    "    file=open(fileName,'r')\n",
    "    text_data=file.read()\n",
    "    file.close()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Step\n",
    "# Turn a document into tokens after processing it.\n",
    "\n",
    "def clean_document(document,m_type=\"mlp\"):\n",
    "    document=document.lower()\n",
    "    #split the review into tokens by white space\n",
    "    tokens=document.split()\n",
    "    # regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # removetokens which are not alphabetis\n",
    "    if m_type==\"mlp\":\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # remove stop words\n",
    "        ##  A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # remove out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'movie', 'comes', 'along', 'suspect', 'studio', 'every', 'indication', 'stinker', 'everybodys', 'surprise', 'perhaps', 'even', 'studio', 'film', 'becomes', 'critical', 'darling', 'mtv', 'films', 'election', 'high', 'school', 'comedy', 'starring', 'matthew', 'broderick', 'reese', 'witherspoon', 'current', 'example', 'anybody', 'know', 'film', 'existed', 'week', 'opened', 'plot', 'deceptively', 'simple', 'george', 'washington', 'carver', 'high', 'school', 'student', 'elections', 'tracy', 'flick', 'reese', 'witherspoon', 'overachiever', 'hand', 'raised', 'nearly', 'every', 'question', 'way', 'way', 'high', 'mr', 'matthew', 'broderick', 'sick', 'megalomaniac', 'student', 'encourages', 'paul', 'popularbutslow', 'jock', 'run', 'pauls', 'nihilistic', 'sister', 'jumps', 'race', 'well', 'personal', 'reasons', 'dark', 'side', 'sleeper', 'success', 'expectations', 'low', 'going', 'fact', 'quality', 'stuff', 'made', 'reviews', 'even', 'enthusiastic', 'right', 'cant', 'help', 'going', 'baggage', 'glowing', 'reviews', 'contrast', 'negative', 'baggage', 'reviewers', 'likely', 'election', 'good', 'film', 'live', 'hype', 'makes', 'election', 'disappointing', 'contains', 'significant', 'plot', 'details', 'lifted', 'directly', 'rushmore', 'released', 'months', 'earlier', 'similarities', 'staggering', 'tracy', 'flick', 'election', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'max', 'fischer', 'rushmore', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'significant', 'tension', 'election', 'potential', 'relationship', 'teacher', 'student', 'significant', 'tension', 'rushmore', 'potential', 'relationship', 'teacher', 'student', 'tracy', 'flick', 'single', 'parent', 'home', 'contributed', 'drive', 'max', 'fischer', 'single', 'parent', 'home', 'contributed', 'drive', 'male', 'bumbling', 'adult', 'election', 'matthew', 'broderick', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'even', 'gets', 'bee', 'sting', 'male', 'bumbling', 'adult', 'rushmore', 'bill', 'murray', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'gets', 'several', 'bee', 'stings', 'happened', 'individual', 'screenplay', 'rushmore', 'novel', 'election', 'contain', 'many', 'significant', 'plot', 'points', 'yet', 'films', 'probably', 'even', 'aware', 'made', 'two', 'different', 'studios', 'genre', 'high', 'school', 'geeks', 'revenge', 'movie', 'hadnt', 'fully', 'formed', 'yet', 'even', 'strengths', 'election', 'rely', 'upon', 'fantastic', 'performances', 'broderick', 'witherspoon', 'newcomer', 'jessica', 'campbell', 'pauls', 'antisocial', 'sister', 'tammy', 'broderick', 'playing', 'mr', 'rooney', 'role', 'ferris', 'bueller', 'seems', 'fun', 'hes', 'since', 'witherspoon', 'revelation', 'early', 'year', 'comedy', 'teenagers', 'little', 'clout', 'money', 'witherspoon', 'deserves', 'oscar', 'nomination', 'campbells', 'character', 'gets', 'going', 'like', 'fantastic', 'speech', 'gymnasium', 'youre', 'one', 'thing', 'thats', 'bothering', 'since', 'ive', 'seen', 'extraordinary', 'amount', 'sexuality', 'film', 'suppose', 'coming', 'mtv', 'films', 'expect', 'less', 'film', 'starts', 'light', 'airy', 'like', 'sitcom', 'screws', 'tighten', 'tensions', 'mount', 'alexander', 'payne', 'decides', 'add', 'elements', 'frankly', 'distract', 'story', 'bad', 'enough', 'mr', 'doesnt', 'like', 'tracys', 'determination', 'win', 'costs', 'throw', 'studentteacher', 'relationship', 'even', 'theres', 'logical', 'reason', 'mr', 'affair', 'theres', 'lot', 'like', 'election', 'plot', 'similarities', 'rushmore', 'tonal', 'nosedive', 'takes', 'gets', 'explicitly', 'sexdriven', 'mark', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_sentoken/pos/cv001_18431.txt\"\n",
    "text=load_document(filename)\n",
    "tokens=clean_document(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory='txt_sentoken/neg'\n",
    "# for file in os.listdir(directory):\n",
    "#     if file.endswith(\".txt\"):\n",
    "#         doc=load_document(directory+'/'+file)\n",
    "#         print(\"Loaded Document %s\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Document and then add the words to Vocab\n",
    "def document_to_vocabulary(fileName,vocab):\n",
    "    document=load_document(fileName)\n",
    "    tokens=clean_document(document)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Process All the documents in the Directory\n",
    "def process_documents(directory, vocab):\n",
    "    count=0\n",
    "    for fileName in os.listdir(directory):\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            path=directory+\"/\"+fileName\n",
    "            document_to_vocabulary(path,vocab)\n",
    "            count+=1\n",
    "    print(\"Total Number Of Files Processed In {d} = {n}\".format(d=directory,n=count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Files Processed In txt_sentoken/neg = 1000\n",
      "Total Number Of Files Processed In txt_sentoken/pos = 1000\n"
     ]
    }
   ],
   "source": [
    "# Main Function To Process the documents \n",
    "# Global Varaible To Store The Count \n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def develop_vocab():\n",
    "    global vocab\n",
    "#     vocab= Counter()\n",
    "    process_documents('txt_sentoken/neg', vocab)\n",
    "    process_documents('txt_sentoken/pos', vocab)\n",
    "    \n",
    "    \n",
    "    min_occur = 5\n",
    "    \n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "    save_list(tokens,\"vocab.txt\")\n",
    "\n",
    "develop_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "#Check Whether The File is Create or not \n",
    "if \"vocab.txt\" in os.listdir():\n",
    "    print(\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length Of The Vocabulary 46557\n"
     ]
    }
   ],
   "source": [
    "#print Length of the Vocabulary\n",
    "print(\"Total Length Of The Vocabulary %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "# 50 Most Common Words\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Ten Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb5760b198>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7wAAAF3CAYAAACG80dpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHdVJREFUeJzt3Xu4ZWV9H/DvTybejaBObESbMZEqaLzEERUviZJ4S6yagpKaiD40PDaoNUYTjTZYI09jY4I2iVoKGlAjKJGIifUSRTBGuXoBRSsFLxSjY0C8UC/gr3/sNXqcnDMzMmeffc7L5/M885y13vWuNb/9nj1r9nevd69d3R0AAAAYzY0WXQAAAADMg8ALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkDYtuoB5uN3tbtdbtmxZdBkAAADMwfnnn/+V7t68q35DBt4tW7bkvPPOW3QZAAAAzEFVfW53+pnSDAAAwJAEXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMSeAFAABgSAIvAAAAQxJ4AQAAGJLACwAAwJAEXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMadOiC1iU+z7vpEWXsK6d/8dPWXQJAAAAe8QVXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMSeAFAABgSAIvAAAAQxJ4AQAAGJLACwAAwJAEXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMSeAFAABgSAIvAAAAQxJ4AQAAGJLACwAAwJAEXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMSeAFAABgSAIvAAAAQxJ4AQAAGJLACwAAwJAEXgAAAIY018BbVb9dVZ+oqouq6k1VddOqunNVnV1Vn6mqU6rqxlPfm0zrl0zbtyw5zgum9k9X1SPnWTMAAABjmFvgrap9kzwrydbuvkeSvZIcluRlSY7t7v2SXJXkiGmXI5Jc1d13SXLs1C9VdcC0392TPCrJq6pqr3nVDQAAwBjmPaV5U5KbVdWmJDdP8sUkD09y6rT9xCSPn5YfN61n2n5wVdXUfnJ3f7u7L0tySZID51w3AAAAG9zcAm93/98kL0/y+cyC7tVJzk/y1e6+dup2eZJ9p+V9k3xh2vfaqf9tl7Yvsw8AAAAsa55TmvfJ7OrsnZPcIcktkjx6ma69fZcVtq3UvuPfd2RVnVdV523btu36FQ0AAMAw5jml+ReTXNbd27r7u0nemuSgJHtPU5yT5I5JrpiWL09ypySZtt86yZVL25fZ5/u6+7ju3trdWzdv3jyPxwMAAMAGMs/A+/kkD6iqm0+fxT04ySeTnJHkkKnP4UneNi2fPq1n2v6+7u6p/bDpLs53TrJfknPmWDcAAAAD2LTrLtdPd59dVacmuSDJtUk+kuS4JH+X5OSqeunUdsK0ywlJXl9Vl2R2Zfew6TifqKo3ZxaWr01yVHdfN6+6AQAAGMPcAm+SdPfRSY7eofnSLHOX5e7+VpJDVzjOMUmOWfUCAQAAGNa8v5YIAAAAFkLgBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGNNfAW1V7V9WpVfWpqrq4qh5YVbepqvdU1Wemn/tMfauq/ntVXVJVH6+qn1tynMOn/p+pqsPnWTMAAABjmPcV3lcmeWd33y3JvZJcnOT5Sd7b3fslee+0niSPTrLf9OfIJK9Okqq6TZKjk9w/yYFJjt4ekgEAAGAlcwu8VfXjSR6a5IQk6e7vdPdXkzwuyYlTtxOTPH5aflySk3rmw0n2rqqfTPLIJO/p7iu7+6ok70nyqHnVDQAAwBjmeYX3p5NsS/K6qvpIVR1fVbdIcvvu/mKSTD9/Yuq/b5IvLNn/8qltpfYfUlVHVtV5VXXetm3bVv/RAAAAsKHMM/BuSvJzSV7d3fdJ8s38YPrycmqZtt5J+w83dB/X3Vu7e+vmzZuvT70AAAAMZJ6B9/Ikl3f32dP6qZkF4C9NU5Uz/fzykv53WrL/HZNcsZN2AAAAWNHcAm93/1OSL1TVXaemg5N8MsnpSbbfafnwJG+blk9P8pTpbs0PSHL1NOX5XUkeUVX7TDeresTUBgAAACvaNOfjPzPJG6vqxkkuTfK0zEL2m6vqiCSfT3Lo1PcdSR6T5JIk10x9091XVtUfJjl36veS7r5yznUDAACwwc018Hb3R5NsXWbTwcv07SRHrXCc1yZ57epWBwAAwMjm/T28AAAAsBACLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABjSpkUXwNg+/5KfXXQJ69a//oMLF10CAAAMzRVeAAAAhiTwAgAAMCRTmmGDe9CfPWjRJaxbH3zmBxddAgAAC+QKLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIQm8AAAADEngBQAAYEgCLwAAAEMSeAEAABiSwAsAAMCQBF4AAACGJPACAAAwJIEXAACAIe1W4K2qB+1OGwAAAKwXu3uF9892sw0AAADWhU0721hVD0xyUJLNVfWcJZt+PMle8ywMAAAA9sROA2+SGye55dTvVkvav5bkkHkVBQAAAHtqp4G3u89McmZV/WV3f26NagIAAIA9tqsrvNvdpKqOS7Jl6T7d/fB5FAUAAAB7ancD71uSvCbJ8Umum185AAAAsDp2N/Be292vnmslAAAAsIp292uJ3l5Vv1VVP1lVt9n+Z66VAQAAwB7Y3Su8h08/n7ekrZP89OqWAwAAAKtjtwJvd9953oUAAADAatqtwFtVT1muvbtPWt1yAAAAYHXs7pTm+y1ZvmmSg5NckETgBQAAYF3a3SnNz1y6XlW3TvL6uVQEAAAAq2B379K8o2uS7LeahQAAAMBq2t3P8L49s7syJ8leSfZP8uZ5FQUAAAB7anc/w/vyJcvXJvlcd18+h3oAAABgVezWlObuPjPJp5LcKsk+Sb4zz6IAAABgT+1W4K2qJyY5J8mhSZ6Y5OyqOmSehQEAAMCe2N0pzS9Mcr/u/nKSVNXmJH+f5NR5FQYAAAB7Ynfv0nyj7WF38s8/wr4AAACw5nb3Cu87q+pdSd40rT8pyTvmUxIAAADsuZ0G3qq6S5Lbd/fzqupXkzw4SSX5UJI3rkF9AAAAcL3salryK5J8PUm6+63d/Zzu/u3Mru6+Yt7FAQAAwPW1q8C7pbs/vmNjd5+XZMtcKgIAAIBVsKvAe9OdbLvZahYCAAAAq2lXgffcqvrNHRur6ogk58+nJAAAANhzu7pL87OTnFZVT84PAu7WJDdO8oR5FgYAAAB7YqeBt7u/lOSgqnpYkntMzX/X3e+be2UAAACwB3bre3i7+4wkZ8y5FgAAAFg1u/oMLwAAAGxIAi8AAABDmnvgraq9quojVfW30/qdq+rsqvpMVZ1SVTee2m8yrV8ybd+y5BgvmNo/XVWPnHfNAAAAbHxrcYX3PyW5eMn6y5Ic2937JbkqyRFT+xFJruruuyQ5duqXqjogyWFJ7p7kUUleVVV7rUHdAAAAbGBzDbxVdcckv5zk+Gm9kjw8yalTlxOTPH5afty0nmn7wVP/xyU5ubu/3d2XJbkkyYHzrBsAAICNb95XeF+R5HeTfG9av22Sr3b3tdP65Un2nZb3TfKFJJm2Xz31/377MvsAAADAsuYWeKvqV5J8ubvPX9q8TNfexbad7bP07zuyqs6rqvO2bdv2I9cLAADAWOZ5hfdBSf5tVX02ycmZTWV+RZK9q2r79//eMckV0/LlSe6UJNP2Wye5cmn7Mvt8X3cf191bu3vr5s2bV//RAAAAsKHMLfB29wu6+47dvSWzm069r7ufnOSMJIdM3Q5P8rZp+fRpPdP293V3T+2HTXdxvnOS/ZKcM6+6AQAAGMOmXXdZdb+X5OSqemmSjyQ5YWo/Icnrq+qSzK7sHpYk3f2Jqnpzkk8muTbJUd193dqXDQAAwEayJoG3u9+f5P3T8qVZ5i7L3f2tJIeusP8xSY6ZX4UAAACMZi2+hxcAAADWnMALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkDYtugCA9e7Mh/78oktY137+rDMXXQIAwLJc4QUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABD2rToAgDgz3/n7YsuYV17xp88dtElAMCG5AovAAAAQxJ4AQAAGJLACwAAwJAEXgAAAIYk8AIAADAkgRcAAIAhCbwAAAAMyffwAsANxDG/fsiiS1i3XviGUxddAgBz4AovAAAAQ3KFFwBglVx8zPsWXcK6tf8LH77oEoAbIFd4AQAAGJLACwAAwJAEXgAAAIYk8AIAADAkgRcAAIAhze0uzVV1pyQnJflXSb6X5LjufmVV3SbJKUm2JPlskid291VVVUlemeQxSa5J8tTuvmA61uFJXjQd+qXdfeK86gYAYP168YtfvOgS1rXVGJ83v+XAPS9kYE889JxFl8CPYJ5XeK9N8jvdvX+SByQ5qqoOSPL8JO/t7v2SvHdaT5JHJ9lv+nNkklcnyRSQj05y/yQHJjm6qvaZY90AAAAMYG5XeLv7i0m+OC1/vaouTrJvkscl+YWp24lJ3p/k96b2k7q7k3y4qvauqp+c+r6nu69Mkqp6T5JHJXnTvGoHAACYp3ud+q5Fl7BufeyQR67asdbkM7xVtSXJfZKcneT2UxjeHop/Yuq2b5IvLNnt8qltpXYAAABY0dwDb1XdMslfJ3l2d39tZ12XaeudtO/49xxZVedV1Xnbtm27fsUCAAAwjLkG3qr6sczC7hu7+61T85emqcqZfn55ar88yZ2W7H7HJFfspP2HdPdx3b21u7du3rx5dR8IAAAAG87cAu901+UTklzc3X+6ZNPpSQ6flg9P8rYl7U+pmQckuXqa8vyuJI+oqn2mm1U9YmoDAACAFc3tplVJHpTkN5JcWFUfndp+P8kfJXlzVR2R5PNJDp22vSOzryS6JLOvJXpaknT3lVX1h0nOnfq9ZPsNrAAAAGAl87xL8z9k+c/fJsnBy/TvJEetcKzXJnnt6lUHAADA6NbkLs0AAACw1gReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCEJvAAAAAxpwwTeqnpUVX26qi6pqucvuh4AAADWtw0ReKtqryR/keTRSQ5I8mtVdcBiqwIAAGA92xCBN8mBSS7p7ku7+ztJTk7yuAXXBAAAwDq2UQLvvkm+sGT98qkNAAAAllXdvegadqmqDk3yyO7+D9P6byQ5sLufuaTPkUmOnFbvmuTTa17onrldkq8suojBGeO1YZznzxjPnzGeP2O8Nozz/Bnj+TPG87cRx/inunvzrjptWotKVsHlSe60ZP2OSa5Y2qG7j0ty3FoWtZqq6rzu3rroOkZmjNeGcZ4/Yzx/xnj+jPHaMM7zZ4znzxjP38hjvFGmNJ+bZL+qunNV3TjJYUlOX3BNAAAArGMb4gpvd19bVc9I8q4keyV5bXd/YsFlAQAAsI5tiMCbJN39jiTvWHQdc7Rhp2NvIMZ4bRjn+TPG82eM588Yrw3jPH/GeP6M8fwNO8Yb4qZVAAAA8KPaKJ/hBQAAgB+JwLtGqupZVXVxVV1VVc+f2l5cVc9ddG2wFqrq6VX1lEXXsRFU1Temn3eoqlOn5adW1Z8vtjJYW1W1paouWnQd601V7V1VvzUtf/88wfxU1bOr6uaLrmMkXhtvDCv9TjbS+XnDfIZ3AL+V5NHdfdmiC4FF6O7XLLqGjaa7r0hyyKLrANadvTN7XfEq54k18+wkb0hyze7uUFV7dfd18ytpw/PamDXhCu8aqKrXJPnpJKdX1W8vd5Wmqt5fVcdW1VnTu133q6q3VtVnquqla1/1xlNVz6mqi6Y/z57eebq4qv5nVX2iqt5dVTeb+v5MVb2zqs6vqg9U1d0WXf96Mo3dp6rq+Gk831hVv1hVH5yekwdW1W2q6m+q6uNV9eGqumdV3aiqPltVey851iVVdful7xAa/92z0runVfXLVfWhqrpdVW2uqr+uqnOnPw9aRK0bQVX9elWdU1Ufrar/UVVHVdV/W7L9qVX1Zyv03Wtq/0ZVHVNVH5ue97df1ONZj6rqP0/njvdU1Zuq6rlVde9prD5eVadV1T5T35Xa7zuN74eSHLXQB7R+/VGSn5men2/Zfp6YnsN/U1Vvr6rLquoZ0/+NH5nG+jZTP+fgnaiqW1TV303Pw4uq6ugkd0hyRlWdMfX5taq6cNr+siX7fqOqXlJVZyd5UVWdtmTbL1XVW9f8Aa1DtUqvjZf5XT1prR/LelNVv1tVz5qWj62q903LB1fVG3b23F2yfEhV/eUyx96Q52eBdw1099OTXJHkYUmu2knX73T3Q5O8JsnbMnsi3SPJU6vqtnMvdAOrqvsmeVqS+yd5QJLfTLJPkv2S/EV33z3JV5P8u2mX45I8s7vvm+S5SV615kWvf3dJ8sok90xytyT/PsmDMxuv30/yX5J8pLvvOa2f1N3fy+y5+4Qkqar7J/lsd39ph2Mb/+upqp6Q5PlJHtPdX8nsd3Rsd98vs+f38Yusb72qqv2TPCnJg7r73kmuS/KNJL+6pNuTkpyyQt8nT31ukeTD3X2vJGdldq4hSVVtzew5eJ/MxnXrtOmkJL83nSsuTHL0Ltpfl+RZ3f3Atap9A3p+kv8zPT+ft8O2e2R2vj4wyTFJrunu+yT5UJLtHytxDt65RyW5orvv1d33SPKKTK/juvthVXWHJC9L8vAk905yv6p6/LTvLZJc1N33T/KSJPtX1eZp29Mye37f4K3ia+Mdf1fvnG/lG8JZSR4yLW9Ncsuq+rHMXsN9Jis/d3fHhjw/m9K8vpw+/bwwySe6+4tJUlWXJrlTkn9eVGEbwIOTnNbd30yS6R3UhyS5rLs/OvU5P8mWqrplkoOSvKWqtu9/kzWudyO4rLsvTJKq+kSS93Z3V9WFSbYk+alMbyB09/uq6rZVdeskpyT5g8xOiodN699n/PfIwzL7z+sR3f21qe0XkxywZCx/vKpu1d1fX0SB69jBSe6b5NxprG6W5MtJLq2qB2T2IuCuST6Y2Quq5fomyXeS/O20fH6SX1qj+jeCByd5W3f/vySpqrdn9uJ/7+4+c+pzYmb/9m+9m+2vT/LoNXsEYzhj+vf/9aq6Osnbp/YLk9zTOXi3XJjk5dPVr7/t7g8sGaskuV+S93f3tiSpqjcmeWiSv8nsDbK/TpLp/8zXJ/n1qnpdkgfmB286sHt29dr4X/yuFlPmunJ+kvtW1a2SfDvJBZm9dnhIZueDlZ67O7WRz88C7/ry7enn95Ysb1/3u9q5WqF96Thel9kL1xsl+er0zjgr2/E5uPT5uSnJtcvs05ldRbjL9I7245PsOCXf+F9/l2Y2BezfJDlvartRkgduDxmsqJKc2N0v+KHGqiOSPDHJpzJ706xr9sr2X/SdfLd/8H1+18W5eamVzsM/6jF8X+Ke2dW52zl4F7r7f08zxx6T5L9W1bt36LKz5/q3dvjc7usyCxnfSvKW7l7u/05WttPXxsv9rrr7JWtd5HrS3d+tqs9mNqPgH5N8PLM3zH8myecze0N32V2XLN90me0b9vxsSjOjOCvJ46vq5lV1i8ym1C77Lt90Zeyyqjo0SWrmXmtX6jDOyjTNs6p+IclXuvtrUxg4LcmfJrm4u39oZoLx3yOfy2yq6ElVdfep7d1JnrG9Q1V5Ebu89yY5pKp+Iklq9hn0n0ry1szemPm1/GA2wkp92bl/SPLYqrrpdBXxl5N8M8lVVbV9et1vJDmzu69eof2rSa6uqgdP7U8Oy/l6kltdnx2dg3dtmrJ8TXe/IcnLk/xcfnjMz07y8zW7j8JemZ0/zlzuWNNNxa5I8qIkfznn0m9wVvhdMXuN9tzp5weSPD3JR5N8OCs/d79UVftX1Y0yfTRtqY18fvbONEPo7gumD9efMzUdn51/JuTJSV5dVS9K8mNJTk7ysbkWOZ4XJ3ldVX08s7tWHr5k2ylJzk3y1BX2Nf7XU3d/uqqenNl0xMcmeVaSv5h+D5sy+8/t6YuscT3q7k9Oz7d3T/+ZfzfJUd39uar6ZJIDuvucnfXN7A0HVtDd51bV6Zn9W/5cZrMQrs7s3PCamn2ly6WZXXXITtqfluS1VXVNknet4UPYMLr7n2t2E8GLklx8PQ7hHLxzP5vkj6vqe5n9+/+PmU1H/l9V9cXpc7wvSHJGZle93tHdb9vJ8d6YZHN3f3Lehd8ALfe7YhZyX5jkQ939zar6VpIPdPcXd/LcfX5mH9n5QpKLktxymeNuyPNz/WBmFgDA9VdVt+zub0wh9qwkR3b3BYuuCxapZncg/kh3n7DoWuCGyBVeAGC1HFdVB2T2+a8ThV1u6Krq/Mym9v/OomuBGypXeAEAABiSm1YBAAAwJIEXAACAIQm8AAAADEngBYB1pqqOrapnL1l/V1Udv2T9T6rqOdfz2C+uqueuRp0AsN4JvACw/vxjkoOSZPou4NslufuS7Qcl+eCuDlJVe82lOgDYIAReAFh/Ppgp8GYWdC9K8vWq2qeqbpJk/yQfrao/rqqLqurCqnpSklTVL1TVGVX1V0kunNpeWFWfrqq/T3LXtX84ALAYvocXANaZ7r6iqq6tqn+dWfD9UJJ9kzwwydVJPp7kV5LcO8m9MrsCfG5VnTUd4sAk9+juy6rqvkkOS3KfzP7fvyDJ+Wv5eABgUQReAFiftl/lPSjJn2YWeA/KLPD+Y5IHJ3lTd1+X5EtVdWaS+yX5WpJzuvuy6TgPSXJad1+TJFV1+po+CgBYIFOaAWB92v453p/NbErzhzO7wrv987u1k32/ucN6z6NAAFjvBF4AWJ8+mNm05Su7+7ruvjLJ3pmF3g8lOSvJk6pqr6ranOShSc5Z5jhnJXlCVd2sqm6V5LFrUz4ALJ4pzQCwPl2Y2Wdz/2qHtlt291eq6rTMwu/HMruC+7vd/U9VdbelB+nuC6rqlCQfTfK5JB9Yk+oBYB2obrOcAAAAGI8pzQAAAAxJ4AUAAGBIAi8AAABDEngBAAAYksALAADAkAReAAAAhiTwAgAAMCSBFwAAgCH9f+5w2PuuZkBNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabmcdf=pd.DataFrame(data=vocab.most_common(10),columns=['Word','Count'])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x='Word',y=\"Count\",data=vocabmcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words in Vocab.txt is 14803\n"
     ]
    }
   ],
   "source": [
    "#Load The Vocabulary from Vocab.txt File.\n",
    "\n",
    "vocab_data=load_document(\"vocab.txt\")\n",
    "vocab_data=vocab_data.split()\n",
    "vocab=set(vocab_data)\n",
    "print(\"Number of Words in Vocab.txt is %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Negative.txt and Positive.txt from all the review documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review_documents(directory,vocab,train=False,op=0,m_type=\"mlp\"):\n",
    "    lines=[]\n",
    "    count=0\n",
    "    for file in os.listdir(directory):\n",
    "#         print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            if not op:\n",
    "                if train and file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "                if not train and not file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "            count=count+1\n",
    "            file_path=directory + '/' + file\n",
    "            lines_in_file=load_document(file_path)\n",
    "            tokens=clean_document(lines_in_file,m_type)\n",
    "            tokens=[w for w in tokens if w in vocab]\n",
    "            line=' '.join(tokens)\n",
    "        lines.append(line)\n",
    "#         print(lines)\n",
    "    print(\"Number of File Processed in {d} is {n}\".format(d=directory,n=count))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 1000\n",
      "Length of the Negative.txt File is 2124581 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "neg_lines=process_review_documents(\"txt_sentoken/neg\",vocab_data,op=1)\n",
    "save_list(neg_lines,\"negative.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txt\n",
    "negative_lines=load_document(\"negative.txt\")\n",
    "print(\"Length of the Negative.txt File is %s \" %len(negative_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studio attracted many weird bizarre people gates wonder film life death studio centers one boring cl\n"
     ]
    }
   ],
   "source": [
    "print(negative_lines[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive lines from all the documents from \"txt_sentoken/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/pos is 1000\n",
      "Length of the Positive.txt File is 2408780 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "pos_lines=process_review_documents(\"txt_sentoken/pos\",vocab_data,op=1)\n",
    "save_list(pos_lines,\"positive.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txr\n",
    "positive_lines=load_document(\"positive.txt\")\n",
    "print(\"Length of the Positive.txt File is %s \" %len(positive_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david lynchs blue velvet begins ends colorful bright shots flowers happy americans seemingly perfect\n"
     ]
    }
   ],
   "source": [
    "print(positive_lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,op,train=False,m_type=\"mlp\"):\n",
    "    neg=process_review_documents('txt_sentoken/neg', vocab,train,op,m_type)\n",
    "    pos=process_review_documents('txt_sentoken/pos', vocab,train,op,m_type)\n",
    "    docs=neg+pos\n",
    "    print(\"Length of   Negative Files = {n} \\t Positive Files= {p} \\t Doc = {d} \".format(n=len(neg),p=len(pos),d=len(docs)))\n",
    "    labels =[0 for i in range(len(neg))] + [1 for j in range(len(pos))]\n",
    "    return docs,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Dividing 90% 10% Ration 1000 Review Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n"
     ]
    }
   ],
   "source": [
    "# load all training reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab,train=True,op=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "test_docs, ytest = load_clean_dataset(vocab,train=False,op=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenizer To Implement Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To Tokenize\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 14781) (200, 14781)\n"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14781"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words=Xtest.shape[1]\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 25)                369550    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 369,576\n",
      "Trainable params: 369,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 1s 567us/step - loss: 0.6921 - acc: 0.5833\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 1s 370us/step - loss: 0.6871 - acc: 0.6111\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 1s 369us/step - loss: 0.6783 - acc: 0.7839\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 1s 376us/step - loss: 0.6663 - acc: 0.8250\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 1s 371us/step - loss: 0.6507 - acc: 0.8217\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 1s 379us/step - loss: 0.6318 - acc: 0.8928\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 1s 377us/step - loss: 0.6106 - acc: 0.8794\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 1s 377us/step - loss: 0.5874 - acc: 0.9206\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 1s 374us/step - loss: 0.5630 - acc: 0.9272\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 1s 376us/step - loss: 0.5374 - acc: 0.9378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb9e9c9860>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit network\n",
    "model=define_model(n_words)\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 160.00 191.00\" width=\"160pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 156,-187 156,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443796674992 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443796674992</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-73.5 7.5,-109.5 144.5,-109.5 144.5,-73.5 7.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-87.8\">dense_11: Dense</text>\n",
       "</g>\n",
       "<!-- 140443796676448 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443796676448</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-.5 7.5,-36.5 144.5,-36.5 144.5,-.5 7.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-14.8\">dense_12: Dense</text>\n",
       "</g>\n",
       "<!-- 140443796674992&#45;&gt;140443796676448 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443796674992-&gt;140443796676448</title>\n",
       "<path d=\"M76,-73.4551C76,-65.3828 76,-55.6764 76,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-46.5903 76,-36.5904 72.5001,-46.5904 79.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443796675384 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443796675384</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 152,-182.5 152,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-160.8\">140443796675384</text>\n",
       "</g>\n",
       "<!-- 140443796675384&#45;&gt;140443796674992 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443796675384-&gt;140443796674992</title>\n",
       "<path d=\"M76,-146.4551C76,-138.3828 76,-128.6764 76,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-119.5903 76,-109.5904 72.5001,-119.5904 79.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 58.435634\n",
      "Test Accuracy: 83.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc  = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (loss*100))\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment For Reviews For Benchmark Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "#     if percent_pos < 0.5:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#             if percent_pos >0.5 and percent_pos<0.505:\n",
    "#                 return percent_pos, 'NEUTRAL'\n",
    "#             else:\n",
    "#                 return percent_pos, 'POSITIVE'\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: NEGATIVE (57.826%) \n",
      "\n",
      "best movie ever great recommend\n",
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (83.711%) \n",
      "\n",
      "bad movie\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (78.721%) \n",
      "\n",
      "average one one time watch\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE (56.944%) \n",
      "\n",
      "give break top movie time even close bad guy movie one worst characters ever seen good flick tried build love peter parker girl sudden cant please movie become cult movie get good rating people afraid speak truth movie wasnt feel sequel might better dont build character much\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (65.232%) \n",
      "\n",
      "ok greatest comic book movie still prefer original batman movie superman movie cool scenes overall disappointed tobey pretty good kirsten liked virgin suicide wasnt digging role fell flat opinion\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: NEGATIVE (54.071%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLP\n",
    "#\n",
    "m_type=\"mlp\"\n",
    "\n",
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Embedding Layer Model\n",
    "\n",
    "I will test different options to see which gives the best result on this problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN With Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "cnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"cnn\")\n",
    "cnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1244\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in cnn_train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(tokenizer ,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embed_cnn_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     filepath=\"weights.best.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor= 'val_acc' , verbose=1, save_best_only=True,\n",
    "#     mode= max )\n",
    "#     callbacks_list = [checkpoint]\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='cnn_model.png', show_shapes=True)\n",
    "#     return model,callbacks_list\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 14781\n"
     ]
    }
   ],
   "source": [
    "cnn_tokenizer=create_tokenizer(cnn_train_docs)\n",
    "vocabulary_size=len(cnn_tokenizer.word_index)+1\n",
    "print(\"Vocabulary Size %d\"%vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "#     tokens = clean_document(review)\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     line = ' '.join(tokens)\n",
    "#     padded=encode_documents(tokenizer,max_length,[line])\n",
    "#     # predict sentiment\n",
    "#     yhat = model.predict(padded, verbose=0)\n",
    "#     percent_pos = yhat[0,0]\n",
    "#     print(percent_pos)\n",
    "#     if percent_pos < 0.49:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "#             return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "cnn_Xtrain = encode_documents(cnn_tokenizer, max_length, cnn_train_docs)\n",
    "cnn_Xtest = encode_documents(cnn_tokenizer, max_length, cnn_test_docs)\n",
    "\n",
    "print(len(cnn_Xtrain))\n",
    "print(len(cnn_Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1244, 100)         1478100   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1237, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 618, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                197770    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,701,513\n",
      "Trainable params: 1,701,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_cnn_model=define_embed_cnn_model(vocabulary_size,max_length)\n",
    "\n",
    "# embed_cnn_model,callb=define_embed_cnn_model(vocabulary_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 262.00 483.00\" width=\"262pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 258,-479 258,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443357897504 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443357897504</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-365.5 25.5,-401.5 228.5,-401.5 228.5,-365.5 25.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-379.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443357898512 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443357898512</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-292.5 51.5,-328.5 202.5,-328.5 202.5,-292.5 51.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-306.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443357897504&#45;&gt;140443357898512 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443357897504-&gt;140443357898512</title>\n",
       "<path d=\"M127,-365.4551C127,-357.3828 127,-347.6764 127,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-338.5903 127,-328.5904 123.5001,-338.5904 130.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443804161528 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443804161528</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 254,-255.5 254,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-233.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443357898512&#45;&gt;140443804161528 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140443357898512-&gt;140443804161528</title>\n",
       "<path d=\"M127,-292.4551C127,-284.3828 127,-274.6764 127,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-265.5903 127,-255.5904 123.5001,-265.5904 130.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357900304 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140443357900304</title>\n",
       "<polygon fill=\"none\" points=\"56.5,-146.5 56.5,-182.5 197.5,-182.5 197.5,-146.5 56.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-160.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443804161528&#45;&gt;140443357900304 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140443804161528-&gt;140443357900304</title>\n",
       "<path d=\"M127,-219.4551C127,-211.3828 127,-201.6764 127,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-192.5903 127,-182.5904 123.5001,-192.5904 130.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443360553504 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140443360553504</title>\n",
       "<polygon fill=\"none\" points=\"63,-73.5 63,-109.5 191,-109.5 191,-73.5 63,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140443357900304&#45;&gt;140443360553504 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140443357900304-&gt;140443360553504</title>\n",
       "<path d=\"M127,-146.4551C127,-138.3828 127,-128.6764 127,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-119.5903 127,-109.5904 123.5001,-119.5904 130.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357465232 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140443357465232</title>\n",
       "<polygon fill=\"none\" points=\"63,-.5 63,-36.5 191,-36.5 191,-.5 63,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140443360553504&#45;&gt;140443357465232 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140443360553504-&gt;140443357465232</title>\n",
       "<path d=\"M127,-73.4551C127,-65.3828 127,-55.6764 127,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-46.5903 127,-36.5904 123.5001,-46.5904 130.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443357898456 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140443357898456</title>\n",
       "<polygon fill=\"none\" points=\"51,-438.5 51,-474.5 203,-474.5 203,-438.5 51,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-452.8\">140443357898456</text>\n",
       "</g>\n",
       "<!-- 140443357898456&#45;&gt;140443357897504 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443357898456-&gt;140443357897504</title>\n",
       "<path d=\"M127,-438.4551C127,-430.3828 127,-420.6764 127,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-411.5903 127,-401.5904 123.5001,-411.5904 130.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(embed_cnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.6900 - acc: 0.5378\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 18s 10ms/step - loss: 0.5349 - acc: 0.7561\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 15s 8ms/step - loss: 0.1252 - acc: 0.9578\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.0145 - acc: 0.9989\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 16s 9ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 9.2583e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 7.4423e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 18s 10ms/step - loss: 6.2321e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb847312e8>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_cnn_model.fit(cnn_Xtrain, ytrain, epochs=10, verbose=1)\n",
    "\n",
    "# history=embed_cnn_model.fit(cnn_Xtrain, ytrain,validation_split=0.20 ,epochs=10, verbose=0,batch_size=100,callbacks=callb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Of Accuracy And Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# print(h`istory.history.keys())\n",
    "\n",
    "# # history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Cnn Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 4s 2ms/step\n",
      "Train Loss: 0.055193\n",
      "Train Accuracy: 100.000000\n",
      "\n",
      "200/200 [==============================] - 1s 3ms/step\n",
      "Test Loss: 49.719241\n",
      "Test Accuracy: 86.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate Train Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtrain, ytrain, verbose=1)\n",
    "print('Train Loss: %f' % (cnn_loss*100))\n",
    "print('Train Accuracy: %f\\n' % (cnn_acc*100))\n",
    "\n",
    "# evaluate Test Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtest, ytest, verbose=1)\n",
    "print('Test Loss: %f' % (cnn_loss*100))\n",
    "print('Test Accuracy: %f' % (cnn_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('embedd_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "#     if percent_pos < 0.49:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "#             return percent_pos, 'POSITIVE'\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "#     tokens = clean_document(review)\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     line = ' '.join(tokens)\n",
    "#     padded=encode_documents(tokenizer,max_length,[line])\n",
    "#     # predict sentiment\n",
    "#     yhat = model.predict(padded, verbose=0)\n",
    "#     percent_pos = yhat[0,0]\n",
    "#     print(percent_pos)\n",
    "#     if round(percent_pos) == 0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_cnn_model.load_weights(\"weights.best.hdf5\")\n",
    "# embed_cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.5109663\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: POSITIVE (51.097%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.4449024\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (55.510%) \n",
      " \n",
      "give break top movie time even close bad guy movie one worst characters ever seen good flick tried build love peter parker girl sudden cant please movie become cult movie get good rating people afraid speak truth movie wasnt feel sequel might better dont build character much\n",
      "0.18961701\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (81.038%) \n",
      "\n",
      "ok greatest comic book movie still prefer original batman movie superman movie cool scenes overall disappointed tobey pretty good kirsten liked virgin suicide wasnt digging role fell flat opinion\n",
      "0.73974013\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: POSITIVE (73.974%) \n",
      "\n",
      "goodness story ever told film would recommend movie everyone\n",
      "0.4999043\n",
      "Review: [The goodness of the  story ever told on film . I would recommend this movie to everyone .]\n",
      "Sentiment: POSITIVE (49.990%) \n",
      "\n",
      "average one one time watch\n",
      "0.5019089\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE (50.191%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n ' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"The goodness of the  story ever told on film . I would recommend this movie to everyone .\"\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everyone enjoy film love recommended\n",
      "0.513243\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: POSITIVE (51.324%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.4449024\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (55.510%) \n",
      "\n",
      "heart touching making us proud movie acting done awesome movie whole theater emotional end movie would recommend movie everyone\n",
      "0.28799298\n",
      "Review: [Very heart touching and making us proud movie. The acting done by akshay Kumar is awesome in the movie.   The whole theater was emotional at the end of the movie . I would recommend this movie to everyone.]\n",
      "Sentiment: NEGATIVE (71.201%) \n",
      "\n",
      "loved movie movie good would recommend movie everyone\n",
      "0.50684905\n",
      "Review: [I loved This Movie. This Movie is too good ,I would recommend this movie to everyone.]\n",
      "Sentiment: POSITIVE (50.685%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"Very heart touching and making us proud movie. The acting done \\\n",
    "by akshay Kumar is awesome in the movie.   \\\n",
    "The whole theater was emotional at the end of the movie . I would recommend this movie to everyone.\"\n",
    "\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n'  % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie. This Movie is too good ,I would recommend this movie to everyone.'\n",
    "percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Embedding Cnn Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "ncnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"ncnn\")\n",
    "ncnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"ncnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_tokenizer = create_tokenizer(ncnn_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1244\n",
      "Vocabulary size: 14781\n"
     ]
    }
   ],
   "source": [
    "length = max([len(s.split()) for s in ncnn_train_docs])\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(ncnn_tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_Xtrain = encode_documents(ncnn_tokenizer,length, ncnn_train_docs)\n",
    "ncnn_Xtest = encode_documents(ncnn_tokenizer, length,ncnn_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1244)\n",
      "(200, 1244)\n"
     ]
    }
   ],
   "source": [
    "print(ncnn_Xtrain.shape)\n",
    "print(ncnn_Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_ncnn_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # channel 3\n",
    "    inputs4 = Input(shape=(length,))\n",
    "    embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "    conv4 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3,flat4])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3,inputs4], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='ncnn_model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1244, 100)    1478100     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1244, 100)    1478100     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1244, 100)    1478100     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1244, 100)    1478100     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1241, 32)     12832       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1239, 32)     19232       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1237, 32)     25632       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1237, 32)     25632       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1241, 32)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1239, 32)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1237, 32)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1237, 32)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 620, 32)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 619, 32)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 618, 32)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 618, 32)      0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 19840)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 19808)        0           max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 19776)        0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 19776)        0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 79200)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 10)           792010      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            11          dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,787,749\n",
      "Trainable params: 6,787,749\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ncnn_model= define_ncnn_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ncnn_Xtrain.shape)\n",
    "# print(ncnn_Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 1114.00 629.00\" width=\"1114pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 1110,-625 1110,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140443756489080 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140443756489080</title>\n",
       "<polygon fill=\"none\" points=\"47,-584.5 47,-620.5 216,-620.5 216,-584.5 47,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-598.8\">input_10: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443756488632 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140443756488632</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-511.5 25.5,-547.5 237.5,-547.5 237.5,-511.5 25.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-525.8\">embedding_12: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443756489080&#45;&gt;140443756488632 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140443756489080-&gt;140443756488632</title>\n",
       "<path d=\"M131.5,-584.4551C131.5,-576.3828 131.5,-566.6764 131.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-557.5903 131.5,-547.5904 128.0001,-557.5904 135.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755367728 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140443755367728</title>\n",
       "<polygon fill=\"none\" points=\"328,-584.5 328,-620.5 497,-620.5 497,-584.5 328,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-598.8\">input_11: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443755146880 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140443755146880</title>\n",
       "<polygon fill=\"none\" points=\"306.5,-511.5 306.5,-547.5 518.5,-547.5 518.5,-511.5 306.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-525.8\">embedding_13: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443755367728&#45;&gt;140443755146880 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140443755367728-&gt;140443755146880</title>\n",
       "<path d=\"M412.5,-584.4551C412.5,-576.3828 412.5,-566.6764 412.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-557.5903 412.5,-547.5904 409.0001,-557.5904 416.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443766350568 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140443766350568</title>\n",
       "<polygon fill=\"none\" points=\"609,-584.5 609,-620.5 778,-620.5 778,-584.5 609,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-598.8\">input_12: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443766351184 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140443766351184</title>\n",
       "<polygon fill=\"none\" points=\"587.5,-511.5 587.5,-547.5 799.5,-547.5 799.5,-511.5 587.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-525.8\">embedding_14: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443766350568&#45;&gt;140443766351184 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140443766350568-&gt;140443766351184</title>\n",
       "<path d=\"M693.5,-584.4551C693.5,-576.3828 693.5,-566.6764 693.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-557.5903 693.5,-547.5904 690.0001,-557.5904 697.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443358201728 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140443358201728</title>\n",
       "<polygon fill=\"none\" points=\"890,-584.5 890,-620.5 1059,-620.5 1059,-584.5 890,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-598.8\">input_13: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140443358165200 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140443358165200</title>\n",
       "<polygon fill=\"none\" points=\"868.5,-511.5 868.5,-547.5 1080.5,-547.5 1080.5,-511.5 868.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-525.8\">embedding_15: Embedding</text>\n",
       "</g>\n",
       "<!-- 140443358201728&#45;&gt;140443358165200 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140443358201728-&gt;140443358165200</title>\n",
       "<path d=\"M974.5,-584.4551C974.5,-576.3828 974.5,-566.6764 974.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"978.0001,-557.5903 974.5,-547.5904 971.0001,-557.5904 978.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443756490200 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140443756490200</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-438.5 51.5,-474.5 211.5,-474.5 211.5,-438.5 51.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-452.8\">conv1d_12: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443756488632&#45;&gt;140443756490200 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140443756488632-&gt;140443756490200</title>\n",
       "<path d=\"M131.5,-511.4551C131.5,-503.3828 131.5,-493.6764 131.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-484.5903 131.5,-474.5904 128.0001,-484.5904 135.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755145928 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140443755145928</title>\n",
       "<polygon fill=\"none\" points=\"332.5,-438.5 332.5,-474.5 492.5,-474.5 492.5,-438.5 332.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-452.8\">conv1d_13: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443755146880&#45;&gt;140443755145928 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140443755146880-&gt;140443755145928</title>\n",
       "<path d=\"M412.5,-511.4551C412.5,-503.3828 412.5,-493.6764 412.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-484.5903 412.5,-474.5904 409.0001,-484.5904 416.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443766352752 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140443766352752</title>\n",
       "<polygon fill=\"none\" points=\"613.5,-438.5 613.5,-474.5 773.5,-474.5 773.5,-438.5 613.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-452.8\">conv1d_14: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443766351184&#45;&gt;140443766352752 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140443766351184-&gt;140443766352752</title>\n",
       "<path d=\"M693.5,-511.4551C693.5,-503.3828 693.5,-493.6764 693.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-484.5903 693.5,-474.5904 690.0001,-484.5904 697.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443358163184 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140443358163184</title>\n",
       "<polygon fill=\"none\" points=\"894.5,-438.5 894.5,-474.5 1054.5,-474.5 1054.5,-438.5 894.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-452.8\">conv1d_15: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140443358165200&#45;&gt;140443358163184 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140443358165200-&gt;140443358163184</title>\n",
       "<path d=\"M974.5,-511.4551C974.5,-503.3828 974.5,-493.6764 974.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"978.0001,-484.5903 974.5,-474.5904 971.0001,-484.5904 978.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443756574032 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140443756574032</title>\n",
       "<polygon fill=\"none\" points=\"48.5,-365.5 48.5,-401.5 214.5,-401.5 214.5,-365.5 48.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-379.8\">dropout_10: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443756490200&#45;&gt;140443756574032 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140443756490200-&gt;140443756574032</title>\n",
       "<path d=\"M131.5,-438.4551C131.5,-430.3828 131.5,-420.6764 131.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-411.5903 131.5,-401.5904 128.0001,-411.5904 135.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443785659056 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140443785659056</title>\n",
       "<polygon fill=\"none\" points=\"329.5,-365.5 329.5,-401.5 495.5,-401.5 495.5,-365.5 329.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-379.8\">dropout_11: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443755145928&#45;&gt;140443785659056 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140443755145928-&gt;140443785659056</title>\n",
       "<path d=\"M412.5,-438.4551C412.5,-430.3828 412.5,-420.6764 412.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-411.5903 412.5,-401.5904 409.0001,-411.5904 416.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443221123312 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140443221123312</title>\n",
       "<polygon fill=\"none\" points=\"610.5,-365.5 610.5,-401.5 776.5,-401.5 776.5,-365.5 610.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-379.8\">dropout_12: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443766352752&#45;&gt;140443221123312 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140443766352752-&gt;140443221123312</title>\n",
       "<path d=\"M693.5,-438.4551C693.5,-430.3828 693.5,-420.6764 693.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-411.5903 693.5,-401.5904 690.0001,-411.5904 697.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443756672728 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140443756672728</title>\n",
       "<polygon fill=\"none\" points=\"891.5,-365.5 891.5,-401.5 1057.5,-401.5 1057.5,-365.5 891.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-379.8\">dropout_13: Dropout</text>\n",
       "</g>\n",
       "<!-- 140443358163184&#45;&gt;140443756672728 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140443358163184-&gt;140443756672728</title>\n",
       "<path d=\"M974.5,-438.4551C974.5,-430.3828 974.5,-420.6764 974.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"978.0001,-411.5903 974.5,-401.5904 971.0001,-411.5904 978.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443756487120 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140443756487120</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 263,-328.5 263,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.5\" y=\"-306.8\">max_pooling1d_12: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443756574032&#45;&gt;140443756487120 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140443756574032-&gt;140443756487120</title>\n",
       "<path d=\"M131.5,-365.4551C131.5,-357.3828 131.5,-347.6764 131.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"135.0001,-338.5903 131.5,-328.5904 128.0001,-338.5904 135.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443756396952 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140443756396952</title>\n",
       "<polygon fill=\"none\" points=\"281,-292.5 281,-328.5 544,-328.5 544,-292.5 281,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-306.8\">max_pooling1d_13: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443785659056&#45;&gt;140443756396952 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140443785659056-&gt;140443756396952</title>\n",
       "<path d=\"M412.5,-365.4551C412.5,-357.3828 412.5,-347.6764 412.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"416.0001,-338.5903 412.5,-328.5904 409.0001,-338.5904 416.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443221124152 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140443221124152</title>\n",
       "<polygon fill=\"none\" points=\"562,-292.5 562,-328.5 825,-328.5 825,-292.5 562,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-306.8\">max_pooling1d_14: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443221123312&#45;&gt;140443221124152 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140443221123312-&gt;140443221124152</title>\n",
       "<path d=\"M693.5,-365.4551C693.5,-357.3828 693.5,-347.6764 693.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"697.0001,-338.5903 693.5,-328.5904 690.0001,-338.5904 697.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443757863376 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>140443757863376</title>\n",
       "<polygon fill=\"none\" points=\"843,-292.5 843,-328.5 1106,-328.5 1106,-292.5 843,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-306.8\">max_pooling1d_15: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140443756672728&#45;&gt;140443757863376 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140443756672728-&gt;140443757863376</title>\n",
       "<path d=\"M974.5,-365.4551C974.5,-357.3828 974.5,-347.6764 974.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"978.0001,-338.5903 974.5,-328.5904 971.0001,-338.5904 978.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755147048 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>140443755147048</title>\n",
       "<polygon fill=\"none\" points=\"141.5,-219.5 141.5,-255.5 291.5,-255.5 291.5,-219.5 141.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216.5\" y=\"-233.8\">flatten_12: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443756487120&#45;&gt;140443755147048 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140443756487120-&gt;140443755147048</title>\n",
       "<path d=\"M152.5112,-292.4551C163.1364,-283.3299 176.1932,-272.1165 187.7355,-262.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"190.1299,-264.7609 195.4359,-255.5904 185.5692,-259.4505 190.1299,-264.7609\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443765813768 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>140443765813768</title>\n",
       "<polygon fill=\"none\" points=\"365.5,-219.5 365.5,-255.5 515.5,-255.5 515.5,-219.5 365.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440.5\" y=\"-233.8\">flatten_13: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443756396952&#45;&gt;140443765813768 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140443756396952-&gt;140443765813768</title>\n",
       "<path d=\"M419.4214,-292.4551C422.5849,-284.2074 426.4027,-274.2536 429.9152,-265.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"433.2478,-266.1806 433.5612,-255.5904 426.7121,-263.6737 433.2478,-266.1806\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443358201448 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>140443358201448</title>\n",
       "<polygon fill=\"none\" points=\"589.5,-219.5 589.5,-255.5 739.5,-255.5 739.5,-219.5 589.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"664.5\" y=\"-233.8\">flatten_14: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443221124152&#45;&gt;140443358201448 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140443221124152-&gt;140443358201448</title>\n",
       "<path d=\"M686.3315,-292.4551C683.055,-284.2074 679.1007,-274.2536 675.4629,-265.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"678.6313,-263.5917 671.6866,-255.5904 672.1259,-266.1761 678.6313,-263.5917\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755747760 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>140443755747760</title>\n",
       "<polygon fill=\"none\" points=\"842.5,-219.5 842.5,-255.5 992.5,-255.5 992.5,-219.5 842.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"917.5\" y=\"-233.8\">flatten_15: Flatten</text>\n",
       "</g>\n",
       "<!-- 140443757863376&#45;&gt;140443755747760 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140443757863376-&gt;140443755747760</title>\n",
       "<path d=\"M960.4101,-292.4551C953.6276,-283.7686 945.3674,-273.1898 937.9123,-263.642\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"940.5384,-261.3182 931.6253,-255.5904 935.0211,-265.6263 940.5384,-261.3182\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755701248 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>140443755701248</title>\n",
       "<polygon fill=\"none\" points=\"442,-146.5 442,-182.5 663,-182.5 663,-146.5 442,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552.5\" y=\"-160.8\">concatenate_4: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140443755147048&#45;&gt;140443755701248 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>140443755147048-&gt;140443755701248</title>\n",
       "<path d=\"M291.905,-221.1174C341.4371,-210.3559 406.5862,-196.2015 459.3521,-184.7375\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"460.3635,-188.0995 469.3924,-182.5561 458.8773,-181.2591 460.3635,-188.0995\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443765813768&#45;&gt;140443755701248 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>140443765813768-&gt;140443755701248</title>\n",
       "<path d=\"M468.1854,-219.4551C482.724,-209.979 500.718,-198.2508 516.3442,-188.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"518.2785,-190.983 524.7449,-182.5904 514.4562,-185.1186 518.2785,-190.983\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443358201448&#45;&gt;140443755701248 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>140443358201448-&gt;140443755701248</title>\n",
       "<path d=\"M636.8146,-219.4551C622.276,-209.979 604.282,-198.2508 588.6558,-188.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"590.5438,-185.1186 580.2551,-182.5904 586.7215,-190.983 590.5438,-185.1186\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755747760&#45;&gt;140443755701248 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>140443755747760-&gt;140443755701248</title>\n",
       "<path d=\"M842.3143,-222.4629C787.457,-211.4914 712.536,-196.5072 652.6131,-184.5226\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"653.2431,-181.0794 642.7508,-182.5502 651.8702,-187.9434 653.2431,-181.0794\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443755702480 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>140443755702480</title>\n",
       "<polygon fill=\"none\" points=\"484,-73.5 484,-109.5 621,-109.5 621,-73.5 484,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552.5\" y=\"-87.8\">dense_19: Dense</text>\n",
       "</g>\n",
       "<!-- 140443755701248&#45;&gt;140443755702480 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>140443755701248-&gt;140443755702480</title>\n",
       "<path d=\"M552.5,-146.4551C552.5,-138.3828 552.5,-128.6764 552.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"556.0001,-119.5903 552.5,-109.5904 549.0001,-119.5904 556.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140443766963728 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>140443766963728</title>\n",
       "<polygon fill=\"none\" points=\"484,-.5 484,-36.5 621,-36.5 621,-.5 484,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552.5\" y=\"-14.8\">dense_20: Dense</text>\n",
       "</g>\n",
       "<!-- 140443755702480&#45;&gt;140443766963728 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>140443755702480-&gt;140443766963728</title>\n",
       "<path d=\"M552.5,-73.4551C552.5,-65.3828 552.5,-55.6764 552.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"556.0001,-46.5903 552.5,-36.5904 549.0001,-46.5904 556.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(ncnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 49s 27ms/step - loss: 0.6896 - acc: 0.5389\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 47s 26ms/step - loss: 0.3663 - acc: 0.8528\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 46s 26ms/step - loss: 0.0254 - acc: 0.9939\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 55s 30ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 46s 26ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 47s 26ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 46s 26ms/step - loss: 7.5496e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb9c3ca518>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncnn_model.fit( [ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain] , ytrain, epochs=7, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_model.save('ncnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncnn_Xtrain=np.array(ncnn_Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 13s 7ms/step\n",
      "Train Accuracy: 100.00\n",
      "200/200 [==============================] - 1s 7ms/step\n",
      "Test Accuracy: 85.50\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = ncnn_model.evaluate([ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain], ytrain, verbose=1)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = ncnn_model.evaluate([ncnn_Xtest,ncnn_Xtest,ncnn_Xtest,ncnn_Xtest], ytest, verbose=1)\n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ncnn_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict([padded,padded,padded,padded], verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "#         if percent_pos >0.5 and percent_pos<0.5045:\n",
    "#             return percent_pos, 'NEUTRAL'\n",
    "#         else:\n",
    "        return percent_pos, 'POSITIVE'\n",
    "#     if round(percent_pos)==0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.49661228\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: POSITIVE (49.661%) \n",
      "\n",
      "loved would recommend movie loved movie goodness goodness goodness would recommend movie everyone\n",
      "0.5306854\n",
      "Review: [I loved This Movie.I would recommend this movie to everyone.I loved This Movie. GoodNess Goodness Goodness I would recommend this movie to everyone.]\n",
      "Sentiment: POSITIVE (53.069%) \n",
      "\n",
      "bad bad bad hate movie one waste money watching\n",
      "0.34448907\n",
      "Review: [Bad , Bad , Bad I Hate this Movie No One Should waste money watching this]\n",
      "Sentiment: NEGATIVE (65.551%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie.I would recommend this movie to everyone.I loved This Movie. GoodNess Goodness Goodness I would recommend this movie to everyone.'\n",
    "percent, sentiment =  predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Bad , Bad , Bad I Hate this Movie No One Should waste money watching this'\n",
    "percent, sentiment =  predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file1.txt',\n",
       " 'file5.txt',\n",
       " 'file7.txt',\n",
       " 'file3.txt',\n",
       " 'file6.txt',\n",
       " 'file8.txt',\n",
       " 'file0.txt',\n",
       " 'file9.txt',\n",
       " 'file4.txt',\n",
       " 'file2.txt']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('txt_sentoken/mtest/pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested using MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Testing For---MLP-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "txt_sentoken/mtest/pos/file0.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "Sentiment: POSITIVE (59.138%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "Sentiment: POSITIVE (62.065%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file2.txt\n",
      "newest addition superhero movie collection dc comics kind like superman meets big story kid suddenly becomes lighthearted adult superhero laughs feel bit like cgi pretty good definitely flaws heart entertaining enough see rest movie review movie review mom\n",
      "Sentiment: POSITIVE (51.260%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file3.txt\n",
      "pleasantly surprised dc movie actually good especially comparison recent dc movies great really however perfectly fine superhero origin story cheesy expected much humour little held back terms action excitement loved character development felt natural seamless worthwhile movie would happily go see\n",
      "Sentiment: POSITIVE (58.534%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file4.txt\n",
      "movie perfect every way different compared superhero films trailers barely show anything many surprises im talking little cameo im talking main stuff also feel superhero films showing similar stuff want something new enjoy every superhero films anyway definitely even dont like superhero films also love\n",
      "Sentiment: POSITIVE (49.747%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file5.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "Sentiment: NEGATIVE (58.198%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "Sentiment: NEGATIVE (52.675%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file7.txt\n",
      "first heard movie trailer cinema waiting captain marvel moment thought one superhero movie dont want watch dont consider dc marvel movie definitely left mark heart story scenes made hard hold tears watching alone id probably character development could done better like power transfer scene feel bit rushed flawless movie havent felt way recent movies since star wars last jedi even took time write review something dont usually bother hence give made feel oh mention amazing comedy laughed hysterically mr moran jokes made really well thought wasnt vulgar ones probably minimal goes show amazing comedy without resort like lot one takes cake\n",
      "Sentiment: NEGATIVE (57.497%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file8.txt\n",
      "love movie kids becoming heroes game also love storyline recommended everybody watch lot fun\n",
      "Sentiment: POSITIVE (69.121%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file9.txt\n",
      "best movie dc extended universe acting performances great visual effects great screenplay great word fantastic\n",
      "Sentiment: POSITIVE (83.362%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "txt_sentoken/mtest/neg/file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "Sentiment: NEGATIVE (60.270%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "Sentiment: NEGATIVE (66.214%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "Sentiment: NEGATIVE (64.550%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "Sentiment: NEGATIVE (66.227%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "Sentiment: NEGATIVE (64.953%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "Sentiment: NEGATIVE (55.740%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "Sentiment: NEGATIVE (62.860%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "Sentiment: NEGATIVE (62.860%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "Sentiment: NEGATIVE (52.450%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "Sentiment: NEGATIVE (58.947%) \n",
      "\n",
      "---------Positive List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  POSITIVE\n",
      "1  file1.txt  POSITIVE\n",
      "2  file2.txt  POSITIVE\n",
      "3  file3.txt  POSITIVE\n",
      "4  file4.txt  POSITIVE\n",
      "5  file5.txt  NEGATIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  POSITIVE\n",
      "9  file9.txt  POSITIVE\n",
      "---------Negative List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  NEGATIVE\n",
      "1  file1.txt  NEGATIVE\n",
      "2  file2.txt  NEGATIVE\n",
      "3  file3.txt  NEGATIVE\n",
      "4  file4.txt  NEGATIVE\n",
      "5  file5.txt  NEGATIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  NEGATIVE\n",
      "9  file9.txt  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderMLP(vocab,tokenizer,model):\n",
    "    print(\"-------------Testing For---MLP-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([files,sentiment])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([files,sentiment])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list,columns=['File Name','Analysis'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['File Name','Analysis'])\n",
    "    print(ndf.head(n=10))\n",
    "    \n",
    "testFromFolderMLP(vocab,tokenizer,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested Using CNN 1 Layer Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing For---CNN 1 Layer-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "-----------Processing ----------file0.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "0.99995303\n",
      "Sentiment: POSITIVE (99.995%) \n",
      "\n",
      "-----------Processing ----------file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "0.89265907\n",
      "Sentiment: POSITIVE (89.266%) \n",
      "\n",
      "-----------Processing ----------file2.txt\n",
      "newest addition superhero movie collection dc comics kind like superman meets big story kid suddenly becomes lighthearted adult superhero laughs feel bit like cgi pretty good definitely flaws heart entertaining enough see rest movie review movie review mom\n",
      "0.88672966\n",
      "Sentiment: POSITIVE (88.673%) \n",
      "\n",
      "-----------Processing ----------file3.txt\n",
      "pleasantly surprised dc movie actually good especially comparison recent dc movies great really however perfectly fine superhero origin story cheesy expected much humour little held back terms action excitement loved character development felt natural seamless worthwhile movie would happily go see\n",
      "0.8381403\n",
      "Sentiment: POSITIVE (83.814%) \n",
      "\n",
      "-----------Processing ----------file4.txt\n",
      "movie perfect every way different compared superhero films trailers barely show anything many surprises im talking little cameo im talking main stuff also feel superhero films showing similar stuff want something new enjoy every superhero films anyway definitely even dont like superhero films also love\n",
      "0.072609246\n",
      "Sentiment: NEGATIVE (92.739%) \n",
      "\n",
      "-----------Processing ----------file5.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "0.39873114\n",
      "Sentiment: NEGATIVE (60.127%) \n",
      "\n",
      "-----------Processing ----------file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "0.13423839\n",
      "Sentiment: NEGATIVE (86.576%) \n",
      "\n",
      "-----------Processing ----------file7.txt\n",
      "first heard movie trailer cinema waiting captain marvel moment thought one superhero movie dont want watch dont consider dc marvel movie definitely left mark heart story scenes made hard hold tears watching alone id probably character development could done better like power transfer scene feel bit rushed flawless movie havent felt way recent movies since star wars last jedi even took time write review something dont usually bother hence give made feel oh mention amazing comedy laughed hysterically mr moran jokes made really well thought wasnt vulgar ones probably minimal goes show amazing comedy without resort like lot one takes cake\n",
      "0.05901559\n",
      "Sentiment: NEGATIVE (94.098%) \n",
      "\n",
      "-----------Processing ----------file8.txt\n",
      "love movie kids becoming heroes game also love storyline recommended everybody watch lot fun\n",
      "0.765719\n",
      "Sentiment: POSITIVE (76.572%) \n",
      "\n",
      "-----------Processing ----------file9.txt\n",
      "best movie dc extended universe acting performances great visual effects great screenplay great word fantastic\n",
      "0.89192253\n",
      "Sentiment: POSITIVE (89.192%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "txt_sentoken/mtest/neg/file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "0.0003921597\n",
      "Sentiment: NEGATIVE (99.961%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "0.023352487\n",
      "Sentiment: NEGATIVE (97.665%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "0.033625938\n",
      "Sentiment: NEGATIVE (96.637%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "0.35020414\n",
      "Sentiment: NEGATIVE (64.980%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "0.004413831\n",
      "Sentiment: NEGATIVE (99.559%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "0.40504828\n",
      "Sentiment: NEGATIVE (59.495%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.04266677\n",
      "Sentiment: NEGATIVE (95.733%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.04266677\n",
      "Sentiment: NEGATIVE (95.733%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "0.42865244\n",
      "Sentiment: NEGATIVE (57.135%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "0.030771319\n",
      "Sentiment: NEGATIVE (96.923%) \n",
      "\n",
      "---------Positive List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  POSITIVE\n",
      "1  file1.txt  POSITIVE\n",
      "2  file2.txt  POSITIVE\n",
      "3  file3.txt  POSITIVE\n",
      "4  file4.txt  NEGATIVE\n",
      "5  file5.txt  NEGATIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  POSITIVE\n",
      "9  file9.txt  POSITIVE\n",
      "---------Negative List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  NEGATIVE\n",
      "1  file1.txt  NEGATIVE\n",
      "2  file2.txt  NEGATIVE\n",
      "3  file3.txt  NEGATIVE\n",
      "4  file4.txt  NEGATIVE\n",
      "5  file5.txt  NEGATIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  NEGATIVE\n",
      "9  file9.txt  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderCnn(vocab,tokenizer,l,model):\n",
    "    print(\"------------Testing For---CNN 1 Layer-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(\"-----------Processing ----------%s\"%files)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_cnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([files,sentiment])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_cnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([files,sentiment])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list,columns=['File Name','Analysis'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['File Name','Analysis'])\n",
    "    print(ndf.head(n=10))\n",
    "    \n",
    "testFromFolderCnn(vocab,cnn_tokenizer,max_length,embed_cnn_model)\n",
    "\n",
    "\n",
    "# percent, sentiment = predict_cnn_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tested Using CNN Multichannel Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing For---CNN 1 Layer-----------------------\n",
      "------------Testing For Positive Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "txt_sentoken/mtest/pos/file0.txt\n",
      "directed david lights creation written henry earth echo stars billy film follows billy orphan boy keeps running foster homes gets adopted couple share home many foster kids billy soon granted magical wizard powers discovers says name turns adult male superhero like abilities isnt much franchise point separated films franchise essentially thing follows suit references superman batman justice league heroes throughout film isnt concerned connecting setting franchise tries thing strongest aspect film far underrated actor brings film energy charm humor needs plays role teenage boy stuck mans body well eerily convincingly jack dylan plays billys foster brother freddy also great neurotic hilarious helps convince films ridiculous premise angel plays young billy got short end stick hes given character hides emotions still feels like billy hes less entertaining half billy nonetheless delivers great performance film lands rest cast alright weak performances decent one mark strong script another strength humor drama horror character spectacle better film far cared characters understood motivations desires film character piece superhero film though story isnt boy turns adult superhero battling evil boy learning grow accept things hes ignored whole life billy must realize must pursue needs wants comes understand everything believed true wrong fantastic makes entire third act land movie carries tongueincheek references never forgets make laugh humor natural fresh never undermines dramatic tension film direction strong definitely sequences ill see second time really analyze film shot remember correctly rarely goes large movie doesnt try spectacle instead remains small unlike hero score great soundtrack amazing genuine laughs film definitely one set piece long could third act dont many complaints outside also brings spin superhero genre definitely family superhero film would argue even family superhero film aside mild language couple scary scenes earns rating one wholesome superhero movies come along loved film everybody knocked park heart humor great message hands best film cant wait see future movies far favorite film year\n",
      "0.9788098\n",
      "Sentiment: POSITIVE (97.881%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file1.txt\n",
      "stars superhero incarnation average superhero blockbuster teenage foster child philadelphia discovers calling superhero angel solid teenage considerably upstaged jack dylan gives likeable performance aspiring sidekick mark strong menacing human vessel seven deadly sins rousing funny film great entertainment although wonder much medium children truly many tense scenes mayhem portrayal seven deadly sins might scare children aspect foster care portrayed adds films film take take shape keeping mindset teenage boy beginning discover true potential payoff tremendous goes decent superior becomes classic battle good evil welcome addition superhero genre gladly recommended\n",
      "0.67358005\n",
      "Sentiment: POSITIVE (67.358%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file2.txt\n",
      "newest addition superhero movie collection dc comics kind like superman meets big story kid suddenly becomes lighthearted adult superhero laughs feel bit like cgi pretty good definitely flaws heart entertaining enough see rest movie review movie review mom\n",
      "0.5543976\n",
      "Sentiment: POSITIVE (55.440%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file3.txt\n",
      "pleasantly surprised dc movie actually good especially comparison recent dc movies great really however perfectly fine superhero origin story cheesy expected much humour little held back terms action excitement loved character development felt natural seamless worthwhile movie would happily go see\n",
      "0.58336085\n",
      "Sentiment: POSITIVE (58.336%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file4.txt\n",
      "movie perfect every way different compared superhero films trailers barely show anything many surprises im talking little cameo im talking main stuff also feel superhero films showing similar stuff want something new enjoy every superhero films anyway definitely even dont like superhero films also love\n",
      "0.35744447\n",
      "Sentiment: NEGATIVE (64.256%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file5.txt\n",
      "always rate things first written review everyone go watch seriously would death inside enjoy movie funny sad heartwarming enough action superhero movie couple things probably scary kids unless child like daughter didnt think scary twist well unexpected course go watch\n",
      "0.5007455\n",
      "Sentiment: POSITIVE (50.075%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file6.txt\n",
      "saw trailer thought genius superhero thats actually kid think tom hanks big meets superman remember liking kid took long time call capt marvel much else remember watched serial impressed watched tv show although cheesy entertaining moved onto series fell love renaissance happened months ago elements mixed together bad guys look awesome movie lots laughs tons heart one bad thing short scene parent scene gets dark made feel kinda pulled movie thank god short scene one favorite comic book movies ever batman dark knight superman ca civil war\n",
      "0.44927844\n",
      "Sentiment: NEGATIVE (55.072%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file7.txt\n",
      "first heard movie trailer cinema waiting captain marvel moment thought one superhero movie dont want watch dont consider dc marvel movie definitely left mark heart story scenes made hard hold tears watching alone id probably character development could done better like power transfer scene feel bit rushed flawless movie havent felt way recent movies since star wars last jedi even took time write review something dont usually bother hence give made feel oh mention amazing comedy laughed hysterically mr moran jokes made really well thought wasnt vulgar ones probably minimal goes show amazing comedy without resort like lot one takes cake\n",
      "0.28714687\n",
      "Sentiment: NEGATIVE (71.285%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file8.txt\n",
      "love movie kids becoming heroes game also love storyline recommended everybody watch lot fun\n",
      "0.5738764\n",
      "Sentiment: POSITIVE (57.388%) \n",
      "\n",
      "txt_sentoken/mtest/pos/file9.txt\n",
      "best movie dc extended universe acting performances great visual effects great screenplay great word fantastic\n",
      "0.601247\n",
      "Sentiment: POSITIVE (60.125%) \n",
      "\n",
      "------------Testing For Negative Files---------------\n",
      "['file0.txt', 'file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt']\n",
      "txt_sentoken/mtest/neg/file0.txt\n",
      "dc fan go see movies buy character posters read comics including reason people become fans certain things things done well time generate fans becomes like dc fear unstoppable however subpar painfully illustrated movie poorly written scenes bit uninspired action sprinkled barely plot billy character development whatsoever overcome villain fight know people saying like movie feel like either arent honest like personal reasons present movie speaking terms technical aspects storytelling isnt effective bored entire time comedy parts made feel though poking asking member yes yes record people criticizing movie bad legitimate criticisms using tactic produce better films thing people want problem built movie around idea strong boy promoted identity film movie ends bad people rush defend dont want idea fail movie secondary vehicle slap real shame could great addition dc universe hope learn something ive heard appears dc actually plans escalate identity politics next phase would absolute disaster pointing box office defense good remember tons paying customers didnt like\n",
      "0.014272793\n",
      "Sentiment: NEGATIVE (98.573%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file1.txt\n",
      "terrible absolutely terrible long funny jokes trailers end amazingly boring take ages present characters main villain awful prefer talk sidekick kid freddy probably annoying character theaters last decade god cgi demons ugly badly designed far worst experience theatre dont waste youre time movie make hopefully massive flop box office even opinion\n",
      "0.062709816\n",
      "Sentiment: NEGATIVE (93.729%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file2.txt\n",
      "yes talking people heres movie deserves rating lack originally annoying title character yes old doesnt change fact dialogue would better fit kevin hart superhero yes mean insult huge dc bad cgi superhero good impression early keanu reeves acting range oh yes went basic idea good kid actors strong ok bad role thats dc always better darker movies\n",
      "0.12523894\n",
      "Sentiment: NEGATIVE (87.476%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file3.txt\n",
      "made main character childish movie cgi heads characters bodies floating head bad\n",
      "0.39886114\n",
      "Sentiment: NEGATIVE (60.114%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file4.txt\n",
      "least completely boring seek nothing find kid movie graphic kids offering new original direction memorable scene dc movie completely empty superhero movie nothing new nothing impressive nothing beautiful comedy maybe laugh although jokes simply references unoriginal jokes saw trailers dont expect anything jokes actor playing billy quite good sympathetic somehow becomes stupid childish transforms lets call already obviously didnt catch wisdom enemy doesnt improve movie nothing interesting movie contains random monsters could movies year already production released hopefully shift come new trailer joker prequel least gives hope\n",
      "0.053836964\n",
      "Sentiment: NEGATIVE (94.616%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file5.txt\n",
      "felt movie audience humour also wasnt like dc movies wasnt dark gloomy wasnt enough action movies spent discovering abilities running away enemy also bit times\n",
      "0.46012092\n",
      "Sentiment: NEGATIVE (53.988%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file6.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.11307503\n",
      "Sentiment: NEGATIVE (88.692%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file7.txt\n",
      "marvel film even worst day solid enough think everyone gets carried away normal dc trash rate lot higher quite surprised turned aswell bad ingredients director pretty much ever done shorts producers consist rocks ex brother spent first rock films coffee geoff johns whos produced dc tripe far despite super suit padding trailer best jokes wasnt half bad\n",
      "0.11307503\n",
      "Sentiment: NEGATIVE (88.692%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file8.txt\n",
      "done captain marvel fun character hes fights crime made look like didnt even stay till end disappointed advice watch serial adventures captain marvel tom tyler capt marvel frank jr billy everything terrific villain called dressed head black mask coat covered white story lie excellent better rubbish tried watch would add alive collect old saturday kids matinee one best special fx brothers howard theodore superior every way\n",
      "0.6356187\n",
      "Sentiment: POSITIVE (63.562%) \n",
      "\n",
      "txt_sentoken/mtest/neg/file9.txt\n",
      "dont know reviewers saw movie feels like dc paid bunch folks give real reviews would chance show film braindead america jokes plot lines arent explicitly explained multiple times genuine heart even single fully developed character everyone fits modern superhero film references made hip woke brief nods made possessed social justice crowd watch watching serious superhero movie decades prior even great ones becomes apparent far weve fallen genre sadly seems like stay\n",
      "0.19218107\n",
      "Sentiment: NEGATIVE (80.782%) \n",
      "\n",
      "---------Positive List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  POSITIVE\n",
      "1  file1.txt  POSITIVE\n",
      "2  file2.txt  POSITIVE\n",
      "3  file3.txt  POSITIVE\n",
      "4  file4.txt  NEGATIVE\n",
      "5  file5.txt  POSITIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  POSITIVE\n",
      "9  file9.txt  POSITIVE\n",
      "---------Negative List----------\n",
      "   File Name  Analysis\n",
      "0  file0.txt  NEGATIVE\n",
      "1  file1.txt  NEGATIVE\n",
      "2  file2.txt  NEGATIVE\n",
      "3  file3.txt  NEGATIVE\n",
      "4  file4.txt  NEGATIVE\n",
      "5  file5.txt  NEGATIVE\n",
      "6  file6.txt  NEGATIVE\n",
      "7  file7.txt  NEGATIVE\n",
      "8  file8.txt  POSITIVE\n",
      "9  file9.txt  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def testFromFolderNCnn(vocab,tokenizer,l,model):\n",
    "    print(\"------------Testing For---CNN 1 Layer-----------------------\")\n",
    "    print(\"------------Testing For Positive Files---------------\")\n",
    "    pos_list=list()\n",
    "    neg_list=list()\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/pos'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/pos/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_ncnn_sentiment(text, vocab, tokenizer, l, model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        pos_list.append([files,sentiment])\n",
    "    print(\"------------Testing For Negative Files---------------\")\n",
    "    filesinfolder=sorted(os.listdir('txt_sentoken/mtest/neg'))\n",
    "    print(filesinfolder)\n",
    "    for files in filesinfolder:\n",
    "        filename=\"txt_sentoken/mtest/neg/\"+files\n",
    "        print(filename)\n",
    "        text=load_document(filename)\n",
    "        percent, sentiment = predict_ncnn_sentiment(text, vocab, tokenizer, l,model)\n",
    "#         print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "        print('Sentiment: %s (%.3f%%) \\n' % (sentiment, percent*100))\n",
    "        neg_list.append([files,sentiment])\n",
    "    \n",
    "    print(\"---------Positive List----------\")\n",
    "#     print(*pos_list)\n",
    "    pdf = pd.DataFrame(pos_list,columns=['File Name','Analysis'])\n",
    "    print(pdf.head(n=10))\n",
    "    print(\"---------Negative List----------\")\n",
    "#     print(*neg_list)\n",
    "    ndf = pd.DataFrame(neg_list,columns=['File Name','Analysis'])\n",
    "    print(ndf.head(n=10))\n",
    "    \n",
    "testFromFolderNCnn(vocab,ncnn_tokenizer,length,ncnn_model)\n",
    "\n",
    "\n",
    "# text = 'It is an ok movie one time watch'\n",
    "# percent, sentiment = predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Word Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes= ['binary', 'count', 'tfidf', 'freq']\n",
    "# #Prepare Bag - Of - Words Encoding\n",
    "# results=pd.DataFrame()\n",
    "# def prepare_data(train,test,m):\n",
    "#     token=Tokenizer()\n",
    "#     token.fit_on_texts(train)\n",
    "#     Xtrain=token.texts_to_matrix(train, mode=mode)\n",
    "#     Xtest = token.texts_to_matrix(test, mode=mode)\n",
    "#     return Xtrain,Xtest\n",
    "\n",
    "# #Evaluate\n",
    "# def evaluate_mode(Xtrain,ytrain,Xtest,ytest):\n",
    "#     scores=list()\n",
    "#     no_r=10\n",
    "#     for _ in range(no_r):\n",
    "#         model=Sequential() \n",
    "#         model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "#         model.add(Dense(1,activation='sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#         model.fit(Xtrain,ytrain,epochs=10,verbose=1)\n",
    "#         loss,acc=model.evaluate(Xtest,ytest,verbose=1)\n",
    "#         scores.append(acc)\n",
    "#         print(\"%d accuracy : %s \"%((_+1),acc))\n",
    "#     return scores\n",
    "\n",
    "# for mode in modes:\n",
    "#     Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "#     results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.describe())\n",
    "# plt.figure(figsize=(16,6))\n",
    "# results.boxplot()\n",
    "# plt.xlabel(\"Types\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
