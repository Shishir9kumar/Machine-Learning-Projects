{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Of The Important Libraries Required for Solving This Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import graphviz\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_polarity.tar.gz',\n",
       " 'model.png',\n",
       " '.ipynb_checkpoints',\n",
       " 'negative.txt',\n",
       " 'mlp.ipynb',\n",
       " 'Sentiment_Analysis_Udacity.ipynb',\n",
       " 'txt_sentoken',\n",
       " 'vocab.txt',\n",
       " 'review_polarity',\n",
       " 'positive.txt',\n",
       " 'cnn_model.png',\n",
       " 'cnn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step \n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load document into the notebook\n",
    "def load_document(fileName):\n",
    "    file=open(fileName,'r')\n",
    "    text_data=file.read()\n",
    "    file.close()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Step\n",
    "# Turn a document into tokens after processing it.\n",
    "\n",
    "def clean_document(document,m_type=\"mlp\"):\n",
    "    document=document.lower()\n",
    "    #split the review into tokens by white space\n",
    "    tokens=document.split()\n",
    "    # regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # removetokens which are not alphabetis\n",
    "    if m_type==\"mlp\":\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # remove stop words\n",
    "        ##  A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # remove out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'movie', 'comes', 'along', 'suspect', 'studio', 'every', 'indication', 'stinker', 'everybodys', 'surprise', 'perhaps', 'even', 'studio', 'film', 'becomes', 'critical', 'darling', 'mtv', 'films', 'election', 'high', 'school', 'comedy', 'starring', 'matthew', 'broderick', 'reese', 'witherspoon', 'current', 'example', 'anybody', 'know', 'film', 'existed', 'week', 'opened', 'plot', 'deceptively', 'simple', 'george', 'washington', 'carver', 'high', 'school', 'student', 'elections', 'tracy', 'flick', 'reese', 'witherspoon', 'overachiever', 'hand', 'raised', 'nearly', 'every', 'question', 'way', 'way', 'high', 'mr', 'matthew', 'broderick', 'sick', 'megalomaniac', 'student', 'encourages', 'paul', 'popularbutslow', 'jock', 'run', 'pauls', 'nihilistic', 'sister', 'jumps', 'race', 'well', 'personal', 'reasons', 'dark', 'side', 'sleeper', 'success', 'expectations', 'low', 'going', 'fact', 'quality', 'stuff', 'made', 'reviews', 'even', 'enthusiastic', 'right', 'cant', 'help', 'going', 'baggage', 'glowing', 'reviews', 'contrast', 'negative', 'baggage', 'reviewers', 'likely', 'election', 'good', 'film', 'live', 'hype', 'makes', 'election', 'disappointing', 'contains', 'significant', 'plot', 'details', 'lifted', 'directly', 'rushmore', 'released', 'months', 'earlier', 'similarities', 'staggering', 'tracy', 'flick', 'election', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'max', 'fischer', 'rushmore', 'president', 'extraordinary', 'number', 'clubs', 'involved', 'school', 'play', 'significant', 'tension', 'election', 'potential', 'relationship', 'teacher', 'student', 'significant', 'tension', 'rushmore', 'potential', 'relationship', 'teacher', 'student', 'tracy', 'flick', 'single', 'parent', 'home', 'contributed', 'drive', 'max', 'fischer', 'single', 'parent', 'home', 'contributed', 'drive', 'male', 'bumbling', 'adult', 'election', 'matthew', 'broderick', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'even', 'gets', 'bee', 'sting', 'male', 'bumbling', 'adult', 'rushmore', 'bill', 'murray', 'pursues', 'extramarital', 'affair', 'gets', 'caught', 'whole', 'life', 'ruined', 'gets', 'several', 'bee', 'stings', 'happened', 'individual', 'screenplay', 'rushmore', 'novel', 'election', 'contain', 'many', 'significant', 'plot', 'points', 'yet', 'films', 'probably', 'even', 'aware', 'made', 'two', 'different', 'studios', 'genre', 'high', 'school', 'geeks', 'revenge', 'movie', 'hadnt', 'fully', 'formed', 'yet', 'even', 'strengths', 'election', 'rely', 'upon', 'fantastic', 'performances', 'broderick', 'witherspoon', 'newcomer', 'jessica', 'campbell', 'pauls', 'antisocial', 'sister', 'tammy', 'broderick', 'playing', 'mr', 'rooney', 'role', 'ferris', 'bueller', 'seems', 'fun', 'hes', 'since', 'witherspoon', 'revelation', 'early', 'year', 'comedy', 'teenagers', 'little', 'clout', 'money', 'witherspoon', 'deserves', 'oscar', 'nomination', 'campbells', 'character', 'gets', 'going', 'like', 'fantastic', 'speech', 'gymnasium', 'youre', 'one', 'thing', 'thats', 'bothering', 'since', 'ive', 'seen', 'extraordinary', 'amount', 'sexuality', 'film', 'suppose', 'coming', 'mtv', 'films', 'expect', 'less', 'film', 'starts', 'light', 'airy', 'like', 'sitcom', 'screws', 'tighten', 'tensions', 'mount', 'alexander', 'payne', 'decides', 'add', 'elements', 'frankly', 'distract', 'story', 'bad', 'enough', 'mr', 'doesnt', 'like', 'tracys', 'determination', 'win', 'costs', 'throw', 'studentteacher', 'relationship', 'even', 'theres', 'logical', 'reason', 'mr', 'affair', 'theres', 'lot', 'like', 'election', 'plot', 'similarities', 'rushmore', 'tonal', 'nosedive', 'takes', 'gets', 'explicitly', 'sexdriven', 'mark', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "filename=\"txt_sentoken/pos/cv001_18431.txt\"\n",
    "text=load_document(filename)\n",
    "tokens=clean_document(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory='txt_sentoken/neg'\n",
    "# for file in os.listdir(directory):\n",
    "#     if file.endswith(\".txt\"):\n",
    "#         doc=load_document(directory+'/'+file)\n",
    "#         print(\"Loaded Document %s\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Document and then add the words to Vocab\n",
    "def document_to_vocabulary(fileName,vocab):\n",
    "    document=load_document(fileName)\n",
    "    tokens=clean_document(document)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Process All the documents in the Directory\n",
    "def process_documents(directory, vocab):\n",
    "    count=0\n",
    "    for fileName in os.listdir(directory):\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            path=directory+\"/\"+fileName\n",
    "            document_to_vocabulary(path,vocab)\n",
    "            count+=1\n",
    "    print(\"Total Number Of Files Processed In {d} = {n}\".format(d=directory,n=count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Files Processed In txt_sentoken/neg = 1000\n",
      "Total Number Of Files Processed In txt_sentoken/pos = 1000\n"
     ]
    }
   ],
   "source": [
    "# Main Function To Process the documents \n",
    "# Global Varaible To Store The Count \n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def develop_vocab():\n",
    "    global vocab\n",
    "#     vocab= Counter()\n",
    "    process_documents('txt_sentoken/neg', vocab)\n",
    "    process_documents('txt_sentoken/pos', vocab)\n",
    "    \n",
    "    \n",
    "    min_occur = 5\n",
    "    \n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occur]\n",
    "    save_list(tokens,\"vocab.txt\")\n",
    "\n",
    "develop_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "#Check Whether The File is Create or not \n",
    "if \"vocab.txt\" in os.listdir():\n",
    "    print(\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length Of The Vocabulary 46557\n"
     ]
    }
   ],
   "source": [
    "#print Length of the Vocabulary\n",
    "print(\"Total Length Of The Vocabulary %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "# 50 Most Common Words\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Ten Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbdc0ace3c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabmcdf=pd.DataFrame(data=vocab.most_common(10),columns=['Word','Count'])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x='Word',y=\"Count\",data=vocabmcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words in Vocab.txt is 14803\n"
     ]
    }
   ],
   "source": [
    "#Load The Vocabulary from Vocab.txt File.\n",
    "\n",
    "vocab_data=load_document(\"vocab.txt\")\n",
    "vocab_data=vocab_data.split()\n",
    "vocab=set(vocab_data)\n",
    "print(\"Number of Words in Vocab.txt is %s\" %len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Negative.txt and Positive.txt from all the review documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review_documents(directory,vocab,train=False,op=0,m_type=\"mlp\"):\n",
    "    lines=[]\n",
    "    count=0\n",
    "    for file in os.listdir(directory):\n",
    "#         print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            if not op:\n",
    "                if train and file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "                if not train and not file.startswith(\"cv9\"):\n",
    "                    continue\n",
    "            count=count+1\n",
    "            file_path=directory + '/' + file\n",
    "            lines_in_file=load_document(file_path)\n",
    "            tokens=clean_document(lines_in_file,m_type)\n",
    "            tokens=[w for w in tokens if w in vocab]\n",
    "            line=' '.join(tokens)\n",
    "        lines.append(line)\n",
    "#         print(lines)\n",
    "    print(\"Number of File Processed in {d} is {n}\".format(d=directory,n=count))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 1000\n",
      "Length of the Negative.txt File is 2124581 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "neg_lines=process_review_documents(\"txt_sentoken/neg\",vocab_data,op=1)\n",
    "save_list(neg_lines,\"negative.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txt\n",
    "negative_lines=load_document(\"negative.txt\")\n",
    "print(\"Length of the Negative.txt File is %s \" %len(negative_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studio attracted many weird bizarre people gates wonder film life death studio centers one boring cl\n"
     ]
    }
   ],
   "source": [
    "print(negative_lines[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive lines from all the documents from \"txt_sentoken/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/pos is 1000\n",
      "Length of the Positive.txt File is 2408780 \n"
     ]
    }
   ],
   "source": [
    "# Negative lines from all the documents from \"txt_sentoken/neg\"\n",
    "pos_lines=process_review_documents(\"txt_sentoken/pos\",vocab_data,op=1)\n",
    "save_list(pos_lines,\"positive.txt\")\n",
    "\n",
    "#Load Negative Lines From negative.txr\n",
    "positive_lines=load_document(\"positive.txt\")\n",
    "print(\"Length of the Positive.txt File is %s \" %len(positive_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david lynchs blue velvet begins ends colorful bright shots flowers happy americans seemingly perfect\n"
     ]
    }
   ],
   "source": [
    "print(positive_lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,op,train=False,m_type=\"mlp\"):\n",
    "    neg=process_review_documents('txt_sentoken/neg', vocab,train,op,m_type)\n",
    "    pos=process_review_documents('txt_sentoken/pos', vocab,train,op,m_type)\n",
    "    docs=neg+pos\n",
    "    print(\"Length of   Negative Files = {n} \\t Positive Files= {p} \\t Doc = {d} \".format(n=len(neg),p=len(pos),d=len(docs)))\n",
    "    labels =[0 for i in range(len(neg))] + [1 for j in range(len(pos))]\n",
    "    return docs,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Dividing 90% 10% Ration 1000 Review Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n"
     ]
    }
   ],
   "source": [
    "# load all training reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab,train=True,op=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "test_docs, ytest = load_clean_dataset(vocab,train=False,op=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenizer To Implement Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To Tokenize\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 14781) (200, 14781)\n"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14781"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words=Xtest.shape[1]\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ashish/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                369550    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 369,576\n",
      "Trainable params: 369,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ashish/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6920 - acc: 0.5667\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6849 - acc: 0.6761\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6724 - acc: 0.7850\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6541 - acc: 0.8961\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6301 - acc: 0.9256\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6018 - acc: 0.9028\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5698 - acc: 0.9272\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5381 - acc: 0.9306\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.5044 - acc: 0.9328\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.4707 - acc: 0.9450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdc0b92cc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit network\n",
    "model=define_model(n_words)\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 160.00 191.00\" width=\"160pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 156,-187 156,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140452959562048 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140452959562048</title>\n",
       "<polygon fill=\"none\" points=\"12,-73.5 12,-109.5 140,-109.5 140,-73.5 12,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140452959560200 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140452959560200</title>\n",
       "<polygon fill=\"none\" points=\"12,-.5 12,-36.5 140,-36.5 140,-.5 12,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140452959562048&#45;&gt;140452959560200 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140452959562048-&gt;140452959560200</title>\n",
       "<path d=\"M76,-73.4551C76,-65.3828 76,-55.6764 76,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-46.5903 76,-36.5904 72.5001,-46.5904 79.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452959560536 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140452959560536</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 152,-182.5 152,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76\" y=\"-160.8\">140452959560536</text>\n",
       "</g>\n",
       "<!-- 140452959560536&#45;&gt;140452959562048 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140452959560536-&gt;140452959562048</title>\n",
       "<path d=\"M76,-146.4551C76,-138.3828 76,-128.6764 76,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"79.5001,-119.5903 76,-109.5904 72.5001,-119.5904 79.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 53.672376\n",
      "Test Accuracy: 86.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc  = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (loss*100))\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment For Reviews For Benchmark Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if percent_pos < 0.5:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "            if percent_pos >0.5 and percent_pos<0.505:\n",
    "                return percent_pos, 'NEUTRAL'\n",
    "            else:\n",
    "                return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: NEGATIVE (68.360%)\n",
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (78.463%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (99.414%)\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: POSITIVE (54.691%)\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (80.926%)\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: NEGATIVE (62.284%)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLP\n",
    "#\n",
    "m_type=\"mlp\"\n",
    "\n",
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Embedding Layer Model\n",
    "\n",
    "I will test different options to see which gives the best result on this problem ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN With Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "cnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"cnn\")\n",
    "cnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1244\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in cnn_train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(tokenizer ,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_embed_cnn_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     filepath=\"weights.best.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor= 'val_acc' , verbose=1, save_best_only=True,\n",
    "#     mode= max )\n",
    "#     callbacks_list = [checkpoint]\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='cnn_model.png', show_shapes=True)\n",
    "#     return model,callbacks_list\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 14781\n"
     ]
    }
   ],
   "source": [
    "cnn_tokenizer=create_tokenizer(cnn_train_docs)\n",
    "vocabulary_size=len(cnn_tokenizer.word_index)+1\n",
    "print(\"Vocabulary Size %d\"%vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "        if percent_pos >0.5 and percent_pos<0.5045:\n",
    "            return percent_pos, 'NEUTRAL'\n",
    "        else:\n",
    "            return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "cnn_Xtrain = encode_documents(cnn_tokenizer, max_length, cnn_train_docs)\n",
    "cnn_Xtest = encode_documents(cnn_tokenizer, max_length, cnn_test_docs)\n",
    "\n",
    "print(len(cnn_Xtrain))\n",
    "print(len(cnn_Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1244, 100)         1478100   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1237, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 618, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                197770    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,701,513\n",
      "Trainable params: 1,701,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_cnn_model=define_embed_cnn_model(vocabulary_size,max_length)\n",
    "\n",
    "# embed_cnn_model,callb=define_embed_cnn_model(vocabulary_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 262.00 483.00\" width=\"262pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 258,-479 258,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140452703534496 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140452703534496</title>\n",
       "<polygon fill=\"none\" points=\"25.5,-365.5 25.5,-401.5 228.5,-401.5 228.5,-365.5 25.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-379.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140452959023056 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140452959023056</title>\n",
       "<polygon fill=\"none\" points=\"51.5,-292.5 51.5,-328.5 202.5,-328.5 202.5,-292.5 51.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-306.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140452703534496&#45;&gt;140452959023056 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140452703534496-&gt;140452959023056</title>\n",
       "<path d=\"M127,-365.4551C127,-357.3828 127,-347.6764 127,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-338.5903 127,-328.5904 123.5001,-338.5904 130.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452961208304 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140452961208304</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 254,-255.5 254,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-233.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140452959023056&#45;&gt;140452961208304 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140452959023056-&gt;140452961208304</title>\n",
       "<path d=\"M127,-292.4551C127,-284.3828 127,-274.6764 127,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-265.5903 127,-255.5904 123.5001,-265.5904 130.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452958383744 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140452958383744</title>\n",
       "<polygon fill=\"none\" points=\"56.5,-146.5 56.5,-182.5 197.5,-182.5 197.5,-146.5 56.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-160.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140452961208304&#45;&gt;140452958383744 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140452961208304-&gt;140452958383744</title>\n",
       "<path d=\"M127,-219.4551C127,-211.3828 127,-201.6764 127,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-192.5903 127,-182.5904 123.5001,-192.5904 130.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452703563448 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140452703563448</title>\n",
       "<polygon fill=\"none\" points=\"63,-73.5 63,-109.5 191,-109.5 191,-73.5 63,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140452958383744&#45;&gt;140452703563448 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140452958383744-&gt;140452703563448</title>\n",
       "<path d=\"M127,-146.4551C127,-138.3828 127,-128.6764 127,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-119.5903 127,-109.5904 123.5001,-119.5904 130.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452703481808 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140452703481808</title>\n",
       "<polygon fill=\"none\" points=\"63,-.5 63,-36.5 191,-36.5 191,-.5 63,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140452703563448&#45;&gt;140452703481808 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140452703563448-&gt;140452703481808</title>\n",
       "<path d=\"M127,-73.4551C127,-65.3828 127,-55.6764 127,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-46.5903 127,-36.5904 123.5001,-46.5904 130.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140452703532928 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140452703532928</title>\n",
       "<polygon fill=\"none\" points=\"51,-438.5 51,-474.5 203,-474.5 203,-438.5 51,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-452.8\">140452703532928</text>\n",
       "</g>\n",
       "<!-- 140452703532928&#45;&gt;140452703534496 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140452703532928-&gt;140452703534496</title>\n",
       "<path d=\"M127,-438.4551C127,-430.3828 127,-420.6764 127,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"130.5001,-411.5903 127,-401.5904 123.5001,-411.5904 130.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of The Defined model\n",
    "SVG(model_to_dot(embed_cnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 13s - loss: 0.6887 - acc: 0.5278\n",
      "Epoch 2/10\n",
      " - 13s - loss: 0.5423 - acc: 0.7561\n",
      "Epoch 3/10\n",
      " - 16s - loss: 0.0979 - acc: 0.9839\n",
      "Epoch 4/10\n",
      " - 12s - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 12s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 12s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 12s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 12s - loss: 8.3523e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 12s - loss: 6.5007e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 12s - loss: 5.1527e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdc0ac75c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_cnn_model.fit(cnn_Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "# history=embed_cnn_model.fit(cnn_Xtrain, ytrain,validation_split=0.20 ,epochs=10, verbose=0,batch_size=100,callbacks=callb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Of Accuracy And Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# print(h`istory.history.keys())\n",
    "\n",
    "# # history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Cnn Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.043896\n",
      "Train Accuracy: 100.000000\n",
      "\n",
      "Test Loss: 37.594240\n",
      "Test Accuracy: 87.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate Train Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtrain, ytrain, verbose=0)\n",
    "print('Train Loss: %f' % (cnn_loss*100))\n",
    "print('Train Accuracy: %f\\n' % (cnn_acc*100))\n",
    "\n",
    "# evaluate Test Accuracy and Loss\n",
    "cnn_loss, cnn_acc  = embed_cnn_model.evaluate(cnn_Xtest, ytest, verbose=0)\n",
    "print('Test Loss: %f' % (cnn_loss*100))\n",
    "print('Test Accuracy: %f' % (cnn_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('embedd_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "        if percent_pos >0.5 and percent_pos<0.5045:\n",
    "            return percent_pos, 'NEUTRAL'\n",
    "        else:\n",
    "            return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "#     tokens = clean_document(review)\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     line = ' '.join(tokens)\n",
    "#     padded=encode_documents(tokenizer,max_length,[line])\n",
    "#     # predict sentiment\n",
    "#     yhat = model.predict(padded, verbose=0)\n",
    "#     percent_pos = yhat[0,0]\n",
    "#     print(percent_pos)\n",
    "#     if round(percent_pos) == 0:\n",
    "#         return (1-percent_pos), 'NEGATIVE'\n",
    "#     return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_cnn_model.load_weights(\"weights.best.hdf5\")\n",
    "# embed_cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.4926184\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: POSITIVE (49.262%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.46639067\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (53.361%) \n",
      " \n",
      "give break top movie time even close bad guy movie one worst characters ever seen good flick tried build love peter parker girl sudden cant please movie become cult movie get good rating people afraid speak truth movie wasnt feel sequel might better dont build character much\n",
      "0.41875106\n",
      "Review: [Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.However, I feel that the sequel might be better because they dont have to build up the character so much..]\n",
      "Sentiment: NEGATIVE (58.125%) \n",
      "\n",
      "ok greatest comic book movie still prefer original batman movie superman movie cool scenes overall disappointed tobey pretty good kirsten liked virgin suicide wasnt digging role fell flat opinion\n",
      "0.37751895\n",
      "Review: [Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, wasnt digging her in this role. Fell flat in my opinion.]\n",
      "Sentiment: NEGATIVE (62.248%) \n",
      "\n",
      "goodness story ever told film\n",
      "0.48716816\n",
      "Review: [The goodness of the  story ever told on film]\n",
      "Sentiment: NEGATIVE (51.283%) \n",
      "\n",
      "average one one time watch\n",
      "0.4792797\n",
      "Review: [An above average one for one time watch.]\n",
      "Sentiment: NEGATIVE (52.072%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n ' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Give me a break. Top 200 movie of all time? Not even close. The bad guy in the movie was one of the worst characters \\\n",
    "I ever seen. It just was not a very good flick. It tried to build up the love between Peter Parker and the girl and then all of a sudden, he just \\\n",
    "cant be with her? Please. This movie will become a cult movie and will get good rating because people will be afraid to speak the truth, which was, this movie wasnt very good.\\\n",
    "However, I feel that the sequel might be better because they dont have to build up the character so much..'\n",
    "\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'Spiderman was just ok. Its not the greatest comic book movie. I still prefer the original Batman movie, Superman and The Daredevil movie. Spiderman had a few cool scenes \\\n",
    "but overall I was disappointed. Tobey was pretty good. Kirsten, uhh, liked her in Virgin Suicide, \\\n",
    "wasnt digging her in this role. Fell flat in my opinion.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"The goodness of the  story ever told on film\"\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'An above average one for one time watch.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everyone enjoy film love recommended\n",
      "0.5001669\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: NEUTRAL (50.017%) \n",
      "\n",
      "bad movie watch sucks\n",
      "0.46639067\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (53.361%) \n",
      "\n",
      "heart touching making us proud movie acting done awesome movie whole theater emotional end movie\n",
      "0.3708135\n",
      "Review: [Very heart touching and making us proud movie. The acting done by akshay Kumar is awesome in the movie.   The whole theater was emotional at the end of the movie.]\n",
      "Sentiment: NEGATIVE (62.919%) \n",
      "\n",
      "loved movie would recommend movie everyone\n",
      "0.49124813\n",
      "Review: [I loved This Movie. I would recommend this movie to everyone.]\n",
      "Sentiment: POSITIVE (49.125%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))\n",
    "\n",
    "text = \"Very heart touching and making us proud movie. The acting done \\\n",
    "by akshay Kumar is awesome in the movie.   \\\n",
    "The whole theater was emotional at the end of the movie.\"\n",
    "\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n'  % (text, sentiment, percent*100))\n",
    "\n",
    "text = 'I loved This Movie. I would recommend this movie to everyone.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, cnn_tokenizer, max_length, embed_cnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Embedding Cnn Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of File Processed in txt_sentoken/neg is 900\n",
      "Number of File Processed in txt_sentoken/pos is 900\n",
      "Length of   Negative Files = 900 \t Positive Files= 900 \t Doc = 1800 \n",
      "Number of File Processed in txt_sentoken/neg is 100\n",
      "Number of File Processed in txt_sentoken/pos is 100\n",
      "Length of   Negative Files = 100 \t Positive Files= 100 \t Doc = 200 \n"
     ]
    }
   ],
   "source": [
    "#Load Train And test Set for Embedding \n",
    "\n",
    "ncnn_train_docs, ytrain = load_clean_dataset(vocab, train=True,op=0,m_type=\"ncnn\")\n",
    "ncnn_test_docs, ytest = load_clean_dataset(vocab,train=False,op=0,m_type=\"ncnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_tokenizer = create_tokenizer(ncnn_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1244\n",
      "Vocabulary size: 14781\n"
     ]
    }
   ],
   "source": [
    "length = max([len(s.split()) for s in ncnn_train_docs])\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(ncnn_tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnn_Xtrain = encode_documents(ncnn_tokenizer,length, ncnn_train_docs)\n",
    "ncnn_Xtest = encode_documents(ncnn_tokenizer, length,ncnn_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_ncnn_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ashish/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1244, 100)    1478100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1244, 100)    1478100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1244, 100)    1478100     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1241, 32)     12832       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1239, 32)     19232       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1237, 32)     25632       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1241, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1239, 32)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1237, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 620, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 619, 32)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 618, 32)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 19840)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 19808)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 19776)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 59424)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           594250      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,086,257\n",
      "Trainable params: 5,086,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ncnn_model= define_ncnn_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 42s 23ms/step - loss: 0.6933 - acc: 0.5278\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 38s 21ms/step - loss: 0.4529 - acc: 0.7983\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 43s 24ms/step - loss: 0.0580 - acc: 0.9833\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 38s 21ms/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 8.7565e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd84655898>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncnn_model.fit( [ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain] , ytrain, epochs=7, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 3 arrays: [array([[ 785, 3381,   36, ...,    0,    0,    0],\n       [ 850,   27, 3566, ...,    0,    0,    0],\n       [ 564, 2688, 7429, ...,    0,    0,    0],\n       ...,\n       [  17, 4361,  123, ...,    0, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d0ceb16cb682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate model on training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mncnn_Xtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mncnn_Xtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mncnn_Xtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# evaluate model on test dataset dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 3 arrays: [array([[ 785, 3381,   36, ...,    0,    0,    0],\n       [ 850,   27, 3566, ...,    0,    0,    0],\n       [ 564, 2688, 7429, ...,    0,    0,    0],\n       ...,\n       [  17, 4361,  123, ...,    0, ..."
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([ncnn_Xtrain,ncnn_Xtrain,ncnn_Xtrain], ytrain, verbose=1)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([ncnn_Xtest,ncnn_Xtest,ncnn_Xtest], ytest, verbose=1)\n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ncnn_sentiment(review, vocab, tokenizer,max_length, model):\n",
    "    tokens = clean_document(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    print(line)\n",
    "    padded=encode_documents(tokenizer,max_length,[line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict([padded,padded,padded], verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    print(percent_pos)\n",
    "    if percent_pos < 0.49:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    else:\n",
    "        if percent_pos >0.5 and percent_pos<0.5045:\n",
    "            return percent_pos, 'NEUTRAL'\n",
    "        else:\n",
    "            return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok movie one time watch\n",
      "0.48107523\n",
      "Review: [It is an ok movie one time watch]\n",
      "Sentiment: NEGATIVE (51.892%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'It is an ok movie one time watch'\n",
    "percent, sentiment = predict_ncnn_sentiment(text, vocab, ncnn_tokenizer, length, ncnn_model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%) \\n' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Word Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes= ['binary', 'count', 'tfidf', 'freq']\n",
    "# #Prepare Bag - Of - Words Encoding\n",
    "# results=pd.DataFrame()\n",
    "# def prepare_data(train,test,m):\n",
    "#     token=Tokenizer()\n",
    "#     token.fit_on_texts(train)\n",
    "#     Xtrain=token.texts_to_matrix(train, mode=mode)\n",
    "#     Xtest = token.texts_to_matrix(test, mode=mode)\n",
    "#     return Xtrain,Xtest\n",
    "\n",
    "# #Evaluate\n",
    "# def evaluate_mode(Xtrain,ytrain,Xtest,ytest):\n",
    "#     scores=list()\n",
    "#     no_r=10\n",
    "#     for _ in range(no_r):\n",
    "#         model=Sequential() \n",
    "#         model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "#         model.add(Dense(1,activation='sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#         model.fit(Xtrain,ytrain,epochs=10,verbose=1)\n",
    "#         loss,acc=model.evaluate(Xtest,ytest,verbose=1)\n",
    "#         scores.append(acc)\n",
    "#         print(\"%d accuracy : %s \"%((_+1),acc))\n",
    "#     return scores\n",
    "\n",
    "# for mode in modes:\n",
    "#     Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "#     results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.describe())\n",
    "# plt.figure(figsize=(16,6))\n",
    "# results.boxplot()\n",
    "# plt.xlabel(\"Types\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
